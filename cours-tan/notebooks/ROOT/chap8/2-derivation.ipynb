{"cells":[{"cell_type":"markdown","source":["# Dérivée numérique\n\n","Soit $y:[a,b]\\rightarrow \\mathbb{R}$, de classe $C^1$ et $t_i\\in[a,b$].\n"],"metadata":{}},{"cell_type":"markdown","source":["La dérivée $y'(t_i)$ est donnée par\n"],"metadata":{}},{"cell_type":"markdown","source":["\n$$\n\\begin{array}{lcl}\ny'(t_i)&=&\\displaystyle\\lim_{h\\rightarrow 0^+}\\frac{y(t_i+h)-y(t_i)}{h},\\\\[2mm]\n&=&\\displaystyle\\lim_{h\\rightarrow 0^+}\\frac{y(t_i)-y(t_i-h)}{h},\\\\[2mm]\n&=&\\displaystyle\\lim_{h\\rightarrow 0}\\frac{y(t_i+h)-y(t_i-h)}{2h}.\n\\end{array}\n$$\n"],"metadata":{}},{"cell_type":"markdown","source":["Soit $a= t_0, t_1, \\ldots, t_n=b$, $n+1$ nœuds équirépartis dans $[a,b]$.\n","On note $h=(b-a)/n$ la distance entre deux nœuds consécutifs.\n"],"metadata":{}},{"cell_type":"markdown","source":["Soit $(Dy)_i$ une approximation de $y'(t_i)$. On appelle\n"],"metadata":{}},{"cell_type":"markdown","source":["* **différence finie progressive**\\\nl&#8217;approximation\n\n  \n$$\n\\label{e:fdp}\n  (Dy)_i^{P} = \\frac{y(t_{i+1}) - y(t_i)}{h}, \\qquad i=0,\\ldots,n-1\n$$\n\n\n* **différence finie rétrograde**\\\nl&#8217;approximation\n\n  \n$$\n\\label{e:fdr}\n  (Dy)_i^{R} = \\frac{y(t_{i}) - y(t_{i-1})}{h}, \\qquad i=1,\\ldots,n\n$$\n\n\n* **différence finie centrée**\\\nl&#8217;approximation\n\n  \n$$\n\\label{e:fdc}\n  (Dy)_i^{C} = \\frac{y(t_{i+1}) - y(t_{i-1})}{2h}, \\qquad i=1,\\ldots,n-1\n$$\n\n\n"],"metadata":{"node_name":"dlist"}},{"cell_type":"markdown","source":["Donnons un exemple de calcul de différences finies: considérons la fonction $f(x)=x + \\exp(-20x^2)\\cos(x)$.\n","On peut évaluer l&#8217;approximation de la dérivée par différence finie en un point donné, ici $x=0.37$, en utilisant les formules précédentes.\n","Nous observons que les différences finies progressive et rétrograde sont moins précises que la différence finie centrée.\n"],"metadata":{}},{"cell_type":"code","execution_count":0,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["import numpy as np\n","import plotly.graph_objects as go\n","\n","# Définir la fonction et sa dérivée\n","f = lambda x: x + np.exp(-20*x**2)*np.cos(x)\n","df = lambda x: 1 - 40*x*np.exp(-20*x**2)*np.cos(x) - np.exp(-20*x**2)*np.sin(x)\n","\n","# Choisir un point autour duquel on va illustrer les approximations de la dérivée\n","x_point = 0.37\n","h = 0.2  # Pas pour les différences finies\n","\n","# Calculer les approximations des dérivées en utilisant les différences finies\n","forward_diff = (f(x_point + h) - f(x_point)) / h\n","backward_diff = (f(x_point) - f(x_point - h)) / h\n","centered_diff = (f(x_point + h) - f(x_point - h)) / (2 * h)\n","exact_diff = df(x_point)\n","\n","# Créer les lignes représentant les pentes des approximations\n","x_forward_tangent = np.array([x_point+h, x_point])\n","forward_tangent = f(x_point) + forward_diff * (x_forward_tangent - x_point)\n","\n","x_backward_tangent = np.array([x_point-h, x_point])\n","backward_tangent = f(x_point) + backward_diff * (x_backward_tangent - x_point)\n","x_centered_tangent = np.array([x_point-h, x_point+h])\n","centered_tangent = f(x_point-h) + centered_diff * (x_centered_tangent - x_point+h)\n","x_tangent = np.array([x_point-h, x_point+h])\n","exact_tangent = f(x_point) + exact_diff * (x_tangent - x_point)\n","# Préparer les données pour le tracé\n","fig = go.Figure()\n","\n","# Tracer la fonction\n","x_values = np.linspace(0, 1, 400)\n","fig.add_trace(go.Scatter(x=x_values, y=f(x_values), mode='lines', name='f(x)'))\n","\n","# Tracer les tangentes pour chaque approximation de la dérivée\n","fig.add_trace(go.Scatter(x=x_forward_tangent, y=forward_tangent, mode='lines', name='Différence finie progressive',\n","                         line=dict(dash='dash', color='red')))\n","fig.add_trace(go.Scatter(x=x_backward_tangent, y=backward_tangent, mode='lines', name='Différence finie rétrograde',\n","                         line=dict(dash='dash', color='blue')))\n","fig.add_trace(go.Scatter(x=x_centered_tangent, y=centered_tangent, mode='lines', name='Différence finie centrée',\n","                         line=dict(dash='dash', color='green')))\n","fig.add_trace(go.Scatter(x=x_tangent, y=exact_tangent, mode='lines', name=\"Dérivée exacte\",\n","                            line=dict(dash='dash', color='black')))\n","\n","# Ajouter un point pour la dérivée exacte\n","fig.add_trace(go.Scatter(x=[x_point,x_point], y=[f(x_point),0], mode='markers', name='Point de tangence',\n","                         marker=dict(color='black', size=10)))\n","fig.add_trace(go.Scatter(x=[x_point+h,x_point+h], y=[f(x_point+h),0], mode='markers', name='x+h',\n","                         marker=dict(color='black', size=10)))\n","fig.add_trace(go.Scatter(x=[x_point-h,x_point-h], y=[f(x_point-h),0], mode='markers', name='x-h',\n","                         marker=dict(color='black', size=10)))\n","\n","\n","\n","# Mise en forme du graphique\n","fig.update_layout(title='Illustration des Approximations de la Dérivée Première',\n","                  xaxis_title='x',\n","                  yaxis_title=\"y et y'\",\n","                  legend_title='Légende')\n","\n","# Afficher le graphique\n","fig.show()\n"]},{"cell_type":"markdown","source":["## Erreur\n\n","Étudions l&#8217;erreur de troncature des différences finies : si $y\\in C^2(\\mathbb{R})$, pour tout $t\\in\\mathbb{R}$, il existe un $\\eta$ entre\n","$t_i$ et $t$ tel que l&#8217;on a le développement de Taylor\n","\n$$\ny(t)=y(t_i)+y'(t_i)(t-t_i)+\\frac{y''(\\eta)}{2}(t-t_i)^2.\n$$\n","* **Pour $t=t_{i+1}$**\\\ndans [[taylor]](#taylor), on obtient\n\n  \n$$\ny(t_{i+1}) - y(t_i) = y'(t_i)h + \\frac{y''(\\eta)}{2}h^2,\n$$\n\n  donc la différence finie progressive est donnée par\n\n  \n$$\n(Dy)_i^{P} = \\frac{y(t_{i+1}) - y(t_i)}{h} = y'(t_i) + \\frac{h}{2}y''(\\eta).\n$$\n\n  En particulier\n\n  \n$$\n|y'(t_i)-(Dy)_i^{P}|\\leq Ch, \\quad\n  \\text{où} \\quad C=\\frac{1}{2}\\max_{t\\in [t_i,t_{i+1}]} |y''(t)|.\n$$\n\n\n* **Pour $t=t_{i-1}$**\\\ndans [[taylor]](#taylor) on obtient\n\n  \n$$\ny(t_{i-1}) - y(t_i) = y'(t_i)(-h) + \\frac{y''(\\eta)}{2}(-h)^2,\n$$\n\n  donc la différence finie rétrograde est donnée par\n\n  \n$$\n(Dy)_i^{R} = \\frac{y(t_i) - y(t_{i-1})}{h} = y'(t_i)-\\frac{h}{2}y''(\\eta).\n$$\n\n  En particulier\n\n  \n$$\n|y'(t_i)-(Dy)_i^{R}|\\leq Ch,\n$$\n\n  où $C=\\frac{1}{2}\\max_{t\\in [t_{i-1},t_i]} |y''(t)|$.\n\n\n* **Pour $t=t_{i+1}$ et $t=t_{i-1}$**\\\navec un\ndéveloppement d&#8217;ordre $2$ (si $y\\in C^3$)\n\n  \n$$\n\\begin{aligned}\n  &y(t_{i+1})=y(t_i)+y'(t_i)\\,h+\\frac{y''(t_i)}{2}\\,h^2+\\frac{y'''(\\eta_1)}{6}\\,h^3, \\\\\n  &y(t_{i-1})=y(t_i)-y'(t_i)\\,h+\\frac{y''(t_i)}{2}\\,h^2-\\frac{y'''(\\eta_2)}{6}\\,h^3,\n\\end{aligned}\n$$\n\n  on obtient\n\n  \n$$\ny(t_{i+1}) - y(t_{i-1}) = 2 y'(t_i) h + \\frac{y'''(\\eta_1)+y'''(\\eta_2)}{6}\\,h^3,\n$$\n\n  d&#8217;où\n\n  \n$$\n(Dy)_i^{C} =  \\frac{y(t_{i+1}) - y(t_{i-1})}{2h} = y'(t_i)+\\frac{y'''(\\eta_1)+y'''(\\eta_2)}{12}\\,h^2.\n$$\n\n  On a donc l&#8217;estimation suivante\n\n  \n$$\n|y'(t_i)-(Dy)_i^{C}|\\leq Ch^2,\n$$\n\n  où $C=\\frac{1}{6}\\max_{t\\in [t_{i-1},t_{i+1}]} |y'''(t)|$.\n\n\n","*Définition: erreur de troncature*\\\n","La différence $\\tau(h) = |y'(t_i)-(Dy)_i^{P}|$ (et celles correspondantes aux autres différences finies) est appelée erreur de troncature au point $t_i$. On dira que $\\tau$ est d&#8217;ordre $p>0$ si on a\n\n","\n$$\n\\tau(h) \\leq C h^p,\n$$\n\n","pour une constante $C>0$.\n","Grâce aux éstimations trouvées, on a\n","\n","- erreurs de troncature des différences finies progressive et rétrograde sont d&#8217;ordre 1;\n","- l&#8217;erreur de troncature de la différence finie centrée est d&#8217;ordre 2.\n","\n","Calculons les erreurs de troncature pour la fonction $f(x)=x + \\exp(-20x^2)\\cos(x)$ en utilisant les formules précédentes.\n"],"metadata":{}},{"cell_type":"code","execution_count":0,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["import numpy as np\n","\n","# Définir la fonction et sa dérivée\n","f = lambda x: x + np.exp(-20*x**2)*np.cos(x)\n","df = lambda x: 1 - 40*x*np.exp(-20*x**2)*np.cos(x) - np.exp(-20*x**2)*np.sin(x)\n","\n","# Choisir un point autour duquel on va illustrer les approximations de la dérivée\n","x_point = 0.37\n","hs = np.array([0.2, 0.1, 0.05, 0.025, 0.0125, 0.0125/2])  # Pas pour les différences finies\n","\n","results = []\n","forward_error = []\n","backward_error = []\n","centered_error = []\n","for i,h in enumerate(hs):\n","    # Calculer les approximations des dérivées en utilisant les différences finies\n","    forward_diff = (f(x_point + h) - f(x_point)) / h\n","    backward_diff = (f(x_point) - f(x_point - h)) / h\n","    centered_diff = (f(x_point + h) - f(x_point - h)) / (2 * h)\n","\n","    # Calculer les erreurs de troncature\n","    forward_error.append(np.abs(forward_diff - df(x_point)))\n","    backward_error.append(np.abs(backward_diff - df(x_point)))\n","    centered_error.append(np.abs(centered_diff - df(x_point)))\n","\n","    results.append([h, forward_error[-1], backward_error[-1], centered_error[-1]])\n","\n","\n","# Afficher les erreurs de troncature pour chaque différence finie\n","from tabulate import tabulate\n","print(tabulate(results, headers=['h', 'Erreur de troncature (progressive)', 'Erreur de troncature (rétrograde)', 'Erreur de troncature (centrée)'], floatfmt=\".2e\"))\n"]},{"cell_type":"markdown","source":["À présent vérifions l&#8217;ordre de convergence\n"],"metadata":{}},{"cell_type":"code","execution_count":0,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["import numpy as np\n","\n","# Définir la fonction et sa dérivée\n","f = lambda x: x + np.exp(-20*x**2)*np.cos(x)\n","df = lambda x: 1 - 40*x*np.exp(-20*x**2)*np.cos(x) - np.exp(-20*x**2)*np.sin(x)\n","\n","# Choisir un point autour duquel on va illustrer les approximations de la dérivée\n","x_point = 0.37\n","hs = np.array([0.2, 0.1, 0.05, 0.025, 0.0125, 0.0125/2])  # Pas pour les différences finies\n","\n","orders = []\n","forward_error = []\n","backward_error = []\n","centered_error = []\n","for i,h in enumerate(hs):\n","    # Calculer les approximations des dérivées en utilisant les différences finies\n","    forward_diff = (f(x_point + h) - f(x_point)) / h\n","    backward_diff = (f(x_point) - f(x_point - h)) / h\n","    centered_diff = (f(x_point + h) - f(x_point - h)) / (2 * h)\n","\n","    # Calculer les erreurs de troncature\n","    forward_error.append(np.abs(forward_diff - df(x_point)))\n","    backward_error.append(np.abs(backward_diff - df(x_point)))\n","    centered_error.append(np.abs(centered_diff - df(x_point)))\n","\n","    if i > 0:\n","        orders.append([h, np.log(forward_error[-2] / forward_error[-1]) / np.log(hs[-2] / hs[-1]),\n","                       np.log(backward_error[-2] / backward_error[-1]) / np.log(hs[-2] / hs[-1]),\n","                       np.log(centered_error[-2] / centered_error[-1]) / np.log(hs[-2] / hs[-1])])\n","\n","\n","# Afficher l'ordre de convergence pour chaque différence finie\n","from tabulate import tabulate\n","print(tabulate(orders, headers=['h', 'Ordre de convergence (progressive)', 'Ordre de convergence (rétrograde)', 'Ordre de convergence (centrée)'], floatfmt=\".2f\"))\n"]},{"cell_type":"markdown","source":["*Important:* nous observons que les erreurs de troncature diminuent lorsque le pas h diminue. De plus, les ordres de convergence sont proches de 1 pour les différences finies progressive et rétrograde, et proche de 2 pour la différence finie centrée comme prévu par la théorie.\n"],"metadata":{"node_name":"admonition"}}],"metadata":{"language_info":{"name":"python","version":"3.9.1"},"kernelspec":{"name":"python3","language":"python"}},"nbformat":4,"nbformat_minor":4}