{"cells":[{"cell_type":"markdown","source":["# Calcul de toutes les valeurs propres\n\n","Deux matrices carrées A et B de même dimension sont dites semblables s&#8217;il existe une matrice $P$ inversible telle que\n"],"metadata":{}},{"cell_type":"markdown","source":["\n$$\n\\mathrm{P}^{-1} \\mathrm{AP}=\\mathrm{B} .\n$$\n"],"metadata":{}},{"cell_type":"markdown","source":["Deux matrices semblables ont les mêmes valeurs propres. En effet, si $\\lambda$ est une valeur propre de $\\mathrm{A}$ et $\\mathbf{x} \\neq \\mathbf{0}$ un vecteur propre associé, on a\n"],"metadata":{}},{"cell_type":"markdown","source":["\n$$\n\\mathrm{BP}^{-1} \\mathbf{x}=\\mathrm{P}^{-1} \\mathrm{~A} \\mathbf{x}=\\lambda \\mathrm{P}^{-1} \\mathbf{x},\n$$\n"],"metadata":{}},{"cell_type":"markdown","source":["ce qui revient à dire que $\\lambda$ est aussi valeur propre de $\\mathrm{B}$ et $\\mathbf{y}=\\mathrm{P}^{-1} \\mathbf{x}$ est un vecteur propre associé.\n"],"metadata":{}},{"cell_type":"markdown","source":["Les méthodes permettant le calcul simultané de toutes les valeurs propres d&#8217;une matrice transforment généralement A (après une infinité d&#8217;itérations) en une matrice semblable diagonale ou triangulaire. Les valeurs propres sont alors simplement les coefficients diagonaux de la matrice obtenue.\n","## Méthode QR pour le calcul des valeurs propres\n\n","Parmi ces méthodes, citons la méthode $Q R$ qui est implémentée dans la fonction `scipy.linalg.qr` et qui est utilisée par la fonction `scipy.linalg.eig`.\n","La commande `D,V=scipy.linalg.eig(A)` renvoie un vecteur $D$ contenant toutes les valeurs propres de $A$ ainsi que les vecteur propres normalisés à droite $V$.\n","Le nom de la méthode $QR$ pour le calcul de valeurs propres provient de l&#8217;utilisation répétée de la factorisation $QR$.\n","Nous ne présentons ici la méthode $QR$ pour les valeurs propres que pour les matrices réelles et sous sa forme la plus simple (dont la convergence n&#8217;est pas toujours garantie).\n","*Note:* ","La factorisation $QR$ d&#8217;une matrice $A$ consiste à écrire $A$ comme le produit d&#8217;une matrice orthogonale $Q$ (i.e. $Q^T Q = Q Q ^T = I$) et d&#8217;une matrice triangulaire supérieure $R$.\n","La factorisation $QR$ est très versatile: c&#8217;est une méthode de décomposition matricielle qui permet de résoudre des systèmes linéaires, de calculer des valeurs propres, de résoudre des problèmes d&#8217;optimisation, etc.\n","Par exemple la factorisation $QR$ est utilisée pour résoudre des systèmes linéaires $A x = b$ de la manière suivante:\n","\n","1. On écrit $A$ comme le produit de $Q$ et $R$ en utilisant la factorisation $QR$, puis on cheche $x$  tel que $A x = Q R x = b$.\n","1. On résout le système $Q c = b$ en calculant $c = Q^T b$.\n","1. On résout le système $R x = c$ pour $x$.\n","\n"],"metadata":{}},{"cell_type":"code","execution_count":0,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["import numpy as np\n","from scipy.linalg import qr\n","\n","alpha = 30\n","A = np.array([[alpha, 2, 3, 13],\n","              [5, 11, 10, 8],\n","              [9, 7, 6, 12],\n","              [4, 14, 15, 1]])\n","\n","b=np.array([1,2,3,4])\n","Q,R=qr(A)\n","# verification de Q\n","print(f\"Q orthogonale? {np.allclose(Q.T@Q, np.eye(4))}\")\n","c=Q.T@b\n","x=np.linalg.solve(R,c)\n","print(x)\n","\n","\n","x1=np.linalg.solve(A,b)\n","print(x1)\n","assert np.allclose(x,x1)\n"]},{"cell_type":"markdown","source":["La factorisation $QR$ est également utilisée pour résoudre des problèmes d&#8217;optimisation. Par exemple, la méthode de résolution des moindres carrés  utilise la factorisation $QR$ pour résoudre le problème d&#8217;optimisation. Voici un exemple simple de résolution de moindres carrés:\n"],"metadata":{}},{"cell_type":"code","execution_count":0,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["import numpy as np\n","from scipy.linalg import qr,solve_triangular\n","\n","A = np.array([[1, 2], [3, 4], [5, 6]])\n","b = np.array([1, 2, 3])\n","\n","Q, R = qr(A)\n","print(f\"Q orthogonale? {np.allclose(Q.T@Q, np.eye(3))}\")\n","print(f\"R triangulaire supérieure? {np.allclose(np.triu(R), R)}\")\n","print(f\"R={R}\")\n","c = Q.T @ b\n","print(f\"c={c}\")\n","# solve the subsystem Rx = c using only n rows of R\n","x = solve_triangular(R[:2], c[:2])\n","print(x)\n","\n","x1 = np.linalg.lstsq(A, b, rcond=None)[0]\n","print(x1)\n","assert np.allclose(x, x1)\n"]},{"cell_type":"markdown","source":["Une presentation plus détaillée est disponible dans la [section ci-dessous.](#_décomposition_qr)\n","Utilisons la méthode $QR$ pour calculer les valeurs propres de la matrice $A$ définie ci-dessous:\n","\n","1. L&#8217;idée consiste à construire une suite de matrices $\\mathrm{A}^{(k)}$, toutes semblables à $\\mathrm{A}$.\n","1. Après avoir posé $\\mathrm{A}^{(0)}=\\mathrm{A}$, on utilise la factorisation $\\mathrm{QR}$ pour calculer les matrices carrées $\\mathrm{Q}^{(k+1)}$ et $\\mathrm{R}^{(k+1)}$ pour $k=0,1, \\ldots$ telles que\n","\n$$\n\\mathrm{Q}^{(k+1)} \\mathrm{R}^{(k+1)}=\\mathrm{A}^{(k)},\n$$\n","1. puis on pose $\\mathrm{A}^{(k+1)}=\\mathrm{R}^{(k+1)} \\mathrm{Q}^{(k+1)}$.\n","\n","*Note:* ","$\\mathrm{A}^{(k)}$ est une matrice de Hessenberg, c&#8217;est-à-dire une matrice dont les éléments en dessous de la première sous-diagonale sont nuls.\n","Elle est semblable à $\\mathrm{A}$ et a les mêmes valeurs propres.\n","Les matrices $\\mathrm{A}^{(k)}, k=0,1,2, \\ldots$ sont toutes semblables, elles ont donc les mêmes valeurs propres que A.\n","De plus, si $\\mathrm{A} \\in \\mathbb{R}^{n \\times n}$ et si ses valeurs propres vérifient $\\left|\\lambda_1\\right|>\\left|\\lambda_2\\right|>\\ldots>\\left|\\lambda_n\\right|$, alors\n","\n$$\n\\lim _{k \\rightarrow+\\infty} \\mathrm{A}^{(k)}=\\mathrm{T}=\\left[\\begin{array}{cccc}\n\\lambda_1 & t_{12} & \\ldots & t_{1 n} \\\\\n0 & \\ddots & \\ddots & \\vdots \\\\\n\\vdots & & \\lambda_{n-1} & t_{n-1, n} \\\\\n0 & \\ldots & 0 & \\lambda_n\n\\end{array}\\right]\n$$\n","La vitesse de décroissance vers zéro des coefficients triangulaires inférieurs, $a_{i, j}^{(k)}, i>j$, quand $k$ tend vers l&#8217;infini, dépend de $\\max _i\\left|\\lambda_{i+1} / \\lambda_i\\right|$.\n","En pratique, on stoppe les itérations quand $\\max _{i>j}\\left|a_{i, j}^{(k)}\\right| \\leq \\epsilon$, où $\\epsilon>0$ est une tolérance fixée.\n","*Note:* Si de plus $\\mathrm{A}$ est symétrique, la suite $\\left\\{\\mathrm{A}^{(k)}\\right\\}$ converge vers une matrice diagonale.\n","## Implémentation basique de la méthode QR\n\n","Le Programme ci-dessous implémente la méthode QR. Les paramètres d&#8217;entrée sont la matrice $A$, la tolérance tol et le nombre maximum d&#8217;itérations nmax.\n"],"metadata":{}},{"cell_type":"code","execution_count":0,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["Unresolved directive in 4-qr.adoc - include::example$tan/eig/qrbasic.py[]\n"]},{"cell_type":"markdown","source":["## Exemple\n\n","*Exemple: famille de matrices $A(\\alpha)$*\\\n","Considérons la matrice A(30) et appelons `qrbasic`  pour calculer ses valeurs propres :\n"],"metadata":{}},{"cell_type":"code","execution_count":0,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["from tan.eig.qrbasic import qrbasic\n","import numpy as np\n","\n","alpha = 30\n","A = np.array([[alpha, 2, 3, 13],\n","              [5, 11, 10, 8],\n","              [9, 7, 6, 12],\n","              [4, 14, 15, 1]])\n","D, iter = qrbasic(A, 1e-14,100)\n","print(f\"La méthode converge en {iter} itérations\")\n","print(D)\n","\n","from scipy.linalg import eig\n","D1, _ = eig(A)\n","\n","#D and D1 should be equal\n","assert np.allclose(D, D1)\n"]},{"cell_type":"markdown","source":["Ces valeurs sont en bon accord avec celles obtenues par la commande `scipy.linalg.eig`.\n","La vitesse de convergence décroît quand des valeurs propres ont des modules presque identiques. C&#8217;est le cas de la matrice correspondant à $\\alpha=-30$ : deux valeurs propres ont à peu près le même module et la méthode a alors besoin de 1149 itérations pour converger avec la même tolérance :\n"],"metadata":{}},{"cell_type":"code","execution_count":0,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["from scipy.linalg import eig\n","from tan.eig.qrbasic import qrbasic\n","import numpy as np\n","\n","alpha = -30\n","A = np.array([[alpha, 2, 3, 13],\n","              [5, 11, 10, 8],\n","              [9, 7, 6, 12],\n","              [4, 14, 15, 1]])\n","D, iter = qrbasic(A, 1e-14, 2000)\n","print(f\"La méthode converge en {iter} itérations\")\n","print(D)\n","\n","D1, _ = eig(A)\n","indices = np.argsort(np.abs(D1))\n","# Réorganiser les valeurs propres et vecteurs propres\n","D1 = D1[indices[::-1]]\n","print(D1)\n","\n","# D and D1 should be equal\n","assert np.allclose(D, D1)\n"]},{"cell_type":"markdown","source":["## Décomposition QR\n\n","*Théorème 8: (Décomposition QR)*\\\n","Soit $A \\in \\mathscr{M}_{m,n}({\\mathbb{R}})$ avec $m \\geqslant n$. Alors il existe $Q \\in O_m({\\mathbb{C}})$ et $R \\in \\mathscr{M}_{m,n}({\\mathbb{R}})$ triangulaire supérieure telles que $A = QR$.\n\n","Si $A$ est inversible (ce qui sous-entend $n=m$), on peut supposer de plus que $R$ est à diagonale strictement positive, et la décomposition est alors unique.\n\n","On a un résultat analogue pour ces matrices complexes.\n","* **Preuve de l’unicité**\\\nSi $A=Q_1 R_1 = Q_2 R_2$, $A$\nétant inversible, $R_1$ et $R_2$ aussi de sorte que\n$B=Q_2^T Q_1 = R_2 R_1^{-1}$ est orthogonale et triangulaire\nsupérieure. Mais alors $B^T B = I_n$ est une décomposition de\nCholevski de $I_n$: par unicité, $B=I_n$ et\n$Q_1 = Q_2$, $R_1=R_2$. ◻\n","On va voir deux méthodes pour construire une décomposition QR.\n","## Méthode de Householder\n\n","*Définition 9*\\\n","Si $u \\in {\\mathbb{R}}^n \\setminus \\{0 \\}$, on note $H_v$ la matrice de réflexion\n","par rapport à $v^\\perp$:\n\n","\n$$\nH_v = I_n - \\frac{2}{\\| v \\|^2} v v^T.\n$$\n\n","Si $H_v(v) = -v$ et si $w \\in v^\\perp$,\n","$H_v(w)=w$.\n","pour obtenir un endomorphisme hermitien unitaire.\n","On pose aussi $H_0 = I_n$.\n\n","*Note:* Remarque: dans le cas complexe, on pose\n$\\displaystyle H_v(u) = u - \\frac{1}{\\|v \\|^2} (u^*v + v^*u)$\n","Soit $A \\in \\mathscr{M}_n({\\mathbb{C}})$.\n","On note $u_1 = (a_{1,1}, \\dots, a_{n,1})^T$ la première\n","colonne de $A$,\n","$\\alpha = \\mathop{\\mathrm{\\mathrm{sgn}}}(a_{1,1}) \\| u_1 \\|$\n","(le signe est pour des raisons de stabilité) et\n","$v_1 = u_1 - \\alpha e_1$ si $u_1$ n’est pas\n","colinéaire à $e_1$ et $v = 0$ sinon.\n","Alors on constate que $H_{v_1} u_1 = \\alpha e_1$. En effet, on\n","a $u_1 = v_1 + \\alpha e_1$ donc\n","\n$$\n\\alpha^2 = \\| u_1 \\|^2 = \\| v_1 \\|^2 + \\alpha^2 + 2 \\alpha v_1^T e_1, \\quad \\text{i.e} \\quad \\| v_1 \\|^2 = - 2 \\alpha v_1^T e_1,\n$$\n","et il vient\n","$\\displaystyle v_1^T u_1 = \\| v_1 \\|^2 + \\alpha v^T e_1 = \\frac{1}{2} \\| v_1 \\|^2$,\n","donc\n","\n$$\n\\begin{aligned}\nH_{v_1} u_1 = u_1 - 2 \\frac{v_1^T u_1}{\\| v_1 \\|^2} v_1 = u_1 - v_1 = \\alpha e_1.\n\\end{aligned}\n$$\n","Ceci signifie que\n","\n$$\n\\label{eq:house_1}\nH_{v_1} A = \\begin{pmatrix}\n\\alpha & * & \\cdots & * \\\\\n0 & & \\\\\n\\vdots & & A_1 \\\\\n0 &\n\\end{pmatrix}\n$$\n","On itère alors sur $A_1$, en multipliant à gauche par des\n","matrices du type latexmath:[H_{v_k} = \\begin{pmatrix}\n","I_{k-1} &amp; 0\\\\\n","0  &amp; H_{\\tilde v_k} \\\\\n","\\end{pmatrix}] à la $k$ème itération, où\n","$\\tilde v_k \\in {\\mathbb{R}}^{m-k+1}$ et $v_k = (0_{k-1}, v_k) \\in {\\mathbb{R}}^m$.\n","On constate alors que $H_{v_{m}} \\cdots H_{v_1} A$ est\n","triangulaire supérieure, et\n","$Q : = (H_{v_{m}} \\cdots H_{v_1})^{-1} = H_{v_1}^T \\cdots H_{v_m}^T \\in O_n({\\mathbb{R}})$ (car c’est un groupe).\n","$Q$ et $R = Q^T A$ conviennent.\n","Si $A$ est inversible, pour obtenir des coefficients diagonaux\n","strictement positifs, on reprend la construction avec cette fois\n","$\\alpha = \\| u_1 \\| >0$ (car $u_1 = Ae_1 \\ne 0$).\n","## Méthode de Givens\n\n","On rappelle que pour $\\theta \\in {\\mathbb{R}}$, la matrice\n","latexmath:[\\begin{pmatrix} \\cos \\theta &amp; - \\sin \\theta \\\\\n","\\sin \\theta &amp; \\cos \\theta\n","\\end{pmatrix}] est la matrice de la rotation d’angle $\\theta$\n","du plan.\n","L’idée est, comme dans la méthode de Householder, d’annuler la première colonne de $A$ grâce à des rotations adaptées. Étant donnés\n","$\\alpha, \\beta \\in {\\mathbb{R}}$ tel que $\\alpha^2 + \\beta^2 =1$, on considère\n","les matrices $R_{p,q}(\\alpha,\\beta))$ dont le coefficient\n","d’indice $(i,j)$ vaut $\\alpha$ si\n","$i=j \\in \\{ p,q \\}$, $\\beta$ si\n","$(i,j) = (p,q)$, $-\\beta$ si\n","$(i,j) = (q,p)$, $1$ si\n","$i=j \\notin \\{ p,q \\}$ et $0$ sinon. Il s’agit de la\n","matrice de rotation d’angle $\\theta$ (tel que\n","$\\cos \\theta = \\alpha$, $\\sin \\theta = \\beta$) dans\n","le plan $\\mathop{\\mathrm{\\mathrm{Vect}}}(e_p,e_q)$.\n","On constate que le coefficient $(2,1)$ de\n","$A_{2,1} = R_{2,1}(\\alpha_2, \\beta_2) A$ est nul, pour\n","\n$$\n\\alpha_2 = \\frac{a_{11}}{\\sqrt{a_{1,1}^2 + a_{2,1}^2}}, \\quad \\beta_2 = \\frac{a_{21}}{\\sqrt{a_{1,1}^2 + a_{2,1}^2}}\n$$\n","(si $a_{11} = a_{21} =0$, on pose\n","$(\\alpha_2, \\beta_2) = (1,0)$. On peut recommencer: on peut\n","choisir $(\\alpha_3,\\beta_3)$ de manière similaire, de sorte\n","que $A_{3,1} = R_{3,1} (\\alpha_3, \\beta_3) A_{2,1}$ ait un\n","coefficient $(3,1)$ nul. Vu la manipulation, la nullité du\n","coefficient $(2,1)$ est préservée.\n","En itérant sur chaque coefficient de la première colonne, on obtient une\n","matrice\n","\n$$\nA_{n,1} = R_{n,1}(\\alpha_n, \\beta_n) \\cdots R_{2,1}(\\alpha_2,\\beta_2) A\n$$\n","dont la première colonne est nulle (excepté le premier coefficient).\n","Comme dans le cas de Householder, on peut alors itérer sur chaque\n","colonne. $SO_n({\\mathbb{R}})$ étant un groupe, le produits des rotations\n","$R_{n,1}(\\alpha_n, \\beta_n) \\cdots R_{2,1}(\\alpha_2,\\beta_2) \\in SO_n({\\mathbb{R}})$, et on obtient ainsi une décomposition QR.\n","## Forme Hessenberg\n\n","*Définition 10*\\\n","Une matrice $(a_{ij})_{ij}$ est dite de\n","Hessenberg si $a_{ij} =0$ dès que $j \\leqslant i-2$,\n","autrement dit, les coefficients non nuls se trouvent sur la 1ère\n","sous-diagonale et au-dessus.\n","L’intérêt essentiel est que toute matrice est semblable à une matrice de Hessenberg, via un changement de base *rapide à calculer*.\n","*Théorème 11*\\\n","Soit $A \\in \\mathscr{M}_n({\\mathbb{C}})$. Il existe $v_1, \\dots, v_{n-1} \\in {\\mathbb{C}}$ tels que\n\n","\n$$\nH_{v_{n-1}} \\cdots H_{v_1} A H_{v_1}  \\cdots H_{v_{n-1}} \\text{ soit Hessenberg}.\n$$\n","Pour la première itération, on utilise la méthode de\n","Householder mais uniquement sur les lignes $2$ à\n","$n$. Autrement dit, on choisit\n","$u_1 = (0, a_{2,1} ,\\dots, a_{n,1})^T$,\n","$\\alpha = \\mathop{\\mathrm{\\mathrm{sgn}}}(a_{2,1}) \\| u_1 \\|$\n","et $v_1 =u_1 - \\alpha e_2$, de sorte que\n\n","\n$$\nH_{v_1} A =  \\begin{pmatrix}\na_{1,1} & a_{1,2} & \\cdots & a_{1,n} \\\\\n\\alpha & \\\\\n0 & & \\\\\n\\vdots & & A_1^* \\\\\n0 &\n\\end{pmatrix}\n$$\n\n","Alors, comme $v_1$ n’a pas de composante sur $e_1$\n","(et que la multiplication à droite opère sur les colonnes)\n","$H_{v_1} A H_{v_1}$ a la même forme (avec une matrice\n","$A_1$ au lieu de $A_1^*$). Ensuite, on itère sur\n","$A_1$.\n","*Note:* La même idée fonctionne avec la méthode de Givens.\n","*Corollaire 12*\\\n","Si $A$ est symétrique (ou hermitienne), on obtient une matrice tridiagonale symétrique (resp. hermitienne) après la mise sous forme Hessenberg précédente.\n","En effet, $H_{v}$ est une matrice orthogonale, donc $H_{v} A H_v$ est encore symétrique, et de même en itérant les composées.\n","## Calcul des valeurs propres\n\n","*Algorithme QR:*\\\n","Soit $A \\in \\mathscr{M}_n({\\mathbb{R}})$.\n\n","On pose $A_1 = A$. Pour $k \\geqslant 1$,\n\n","\n","1. On calcule une décomposition QR de $A_k$:\n$A_k = Q_k R_k$.\n","1. On définit $A_{k+1} = R_k Q_k$.\n","\n","Souvent, on commence par mettre sous forme Hessenberg: on pose $A_1 = P^{-1} A P$, avec $A_1$ Hessenberg.\n","Le but est de gagner en complexité: le calcul d’une décomposition QR est en $O(n^2)$ pour une forme Hessenberg contre $O(n^3)$ en général pour une matrice pleine.\n","*Lemme 13*\\\n","$A_k$ et $A$ sont semblables.\n","En particulier, $\\mathop{\\mathrm{\\mathrm{Sp}}}(A_k) = \\mathop{\\mathrm{\\mathrm{Sp}}}(A)$.\n\n","Si $A_1$ est sous forme Hesseberg, $A_k$ aussi pour $k \\geqslant 1$.\n","En effet, $R_k = Q_k^{-1} A_k = Q_k^T A_k$ donc $A_{k+1} = R_k Q_k = Q_k^{T} A_k Q_k$.\n","Ainsi $A_{k+1}$ est (orthogonalement) semblable à $A_k$.\n\n","Si $A_k$ est sous forme Hessenberg,\n","$Q_k = A_k R_k^{-1}$ également (car $R_k$ est\n","triangulaire), donc $A_{k+1} = R_kQ_k$ aussi.\n\n","On conclut par récurrence.\n","*Théorème 14*\\\n","On suppose que $A \\in \\mathscr{M}_n({\\mathbb{C}})$ vérifie: il existe $P \\in GL_n({\\mathbb{C}})$ qui admet une décomposition LU et\n\n","\n$$\nP^{-1} A P = D : = \\mathop{\\mathrm{\\mathrm{diag}}}(\\lambda_1, \\dots, \\lambda_n),\n$$\n\n","avec $|\\lambda_1| > |\\lambda_2| > \\cdots > |\\lambda_n| >0$.\n","(En particulier, $A$ est inversible et diagonalisable à\n","valeurs propres simples).\n\n","Alors, quand $k \\to +\\infty$,\n","$(A_k)_{i,i} \\to \\lambda_i$ et $(A_k)_{i,j} \\to 0$\n","pour tous $j <i$.\n","Remarquons que $Q_{k+1} R_{k+1} = A_{k+1} = R_k Q_k$.\n\n","Notons $O_k = Q_1 \\cdots Q_k$ et\n","$S_k = R_k \\cdots R_1$. Alors $O_k$ est orthogonale\n","et $R_k$ est triangulaire supérieure,\n","$A_k = O_{k-1}^T A O_{k-1}$ et de plus, on a la décomposition\n","QR\n\n","\n$$\nA^k = O_k S_k.\n$$\n\n","En effet, on a par exemple\n\n","\n$$\n\\begin{gathered}\nO_2 S_2 = Q_1 Q_2 R_2 R_1 = Q_1 (R_1 Q_1) R_1 = (Q_1 R_1) (Q_1 R_1) = A^2 \\\\\nO_3 S_3 = Q_1 Q_2 (Q_3 R_3) R_2 R_1 =  Q_1 Q_2 (R_2 Q_2) R_2 R_1 =  Q_1 (R_1 Q_1) (R_1Q_1)  R_1 = A^3\n\\end{gathered}\n$$\n\n","et ainsi de suite.\n\n","D’autre part, soit $P^{-1} = LU$ la factorisation\n","$LU$ de $P^{-1}$ (qui se déduit de celle de\n","$P$) et $P=QR$ avec $R_{ii}>0$ (possible\n","car $P$ est inversible). Alors on a\n\n","\n$$\n\\begin{aligned}\nA^k = P D^k P^{-1} = QR D^k LU = Q (R D^k L D^{-k}   R^{-1}) R D^k U,\n\\end{aligned}\n$$\n\n","et on calcule que $D^k L D^{-k}$ est triangulaire inférieure,\n","et\n\n","\n$$\n(D^k L D^{-k})_{ij} = \\left( \\frac{\\lambda_i}{\\lambda_j} \\right)^k L_{ij} \\quad \\text{si } i \\geqslant j.\n$$\n\n","Par hypothèse sur les $\\lambda_i$, et comme\n","$L_{ii} =1$, cela entraine\n","$RD^k L D^{-k} R^{-1}= I_n + N_k$ avec $N_k \\to 0$.\n\n","Soit $I_n + N_k = \\tilde Q_k \\tilde R_k$ une factorisation\n","$QR$ avec $(\\tilde R_{k})_{ii}>0$ (possible car\n","$I_n +N_k$ est inversible pour $k$ assez grand).\n\n","$O_n({\\mathbb{R}})$ étant compact, on peut extraire de $\\tilde Q_k$ une sous\n","suite convergente: $\\tilde Q_{\\phi(k)} \\to \\tilde Q \\in O_n({\\mathbb{R}})$. Par conséquent, $\\tilde R_{\\phi(k)} \\to \\tilde Q^T$ est\n","triangulaire supérieure et orthogonale: elle est donc diagonale, à\n","coefficients $\\pm 1$. Mais on a aussi\n","$\\tilde R_{\\phi(k), ii} >0$ donc\n","$\\tilde R_{ii} \\geqslant 0$ et donc\n","$\\tilde R = I_n = \\tilde Q$.\n\n","La limite ne dépendant pas de la sous-suite considérée, on a convergence\n","de toute la suite: $\\tilde Q_k, \\tilde R_k \\to I_n$.\n\n","Alors\n\n","\n$$\nA^k = Q (\\tilde Q_k \\tilde R_k) R D^k U = (Q \\tilde Q_k)  (\\tilde R_k R D^k U).\n$$\n\n","C’est une autre décomposition QR de $A^k$. Par unicité de la\n","décomposition, il existe une matrice\n","$D_k = \\mathop{\\mathrm{\\mathrm{diag}}}(\\varepsilon_{i,k})$ où\n","$\\varepsilon_{i,k} \\in \\{ \\pm 1\\}$ adéquate, telle que\n\n","\n$$\nO_k = Q \\tilde Q_k D_k.\n$$\n\n","Faisons le lien avec $A_k$: comme\n","$A = PDP^{-1} = QR DR^{-1}Q^{-1}$,\n\n","\n$$\n\\begin{aligned}\nA_{k+1} & = O_{k}^T A O_{k} = ( Q \\tilde Q_k D_k)^T (QRDR^{-1}Q^{-1})  Q \\tilde Q_k D_k \\\\\n& = D_k \\tilde Q_k RDR^{-1} \\tilde Q_k D_k\n\\end{aligned}\n$$\n\n","Mais $\\tilde Q_k RDR^{-1} \\tilde Q_k \\to RDR^{-1}$ qui est\n","triangulaire avec la même diagonale que $D$:\n","$\\lambda_1, \\dots, \\lambda_n$. On conclut que\n","$(A_{k})_{ii} \\to \\lambda_i$ et $(A_k)_{ij} \\to 0$\n","pour $i \\geqslant j$.\n","*Note:* ","On constate que la convergence est en $O(\\lambda^k)$ où $\\lambda = \\min |\\lambda_{k+1}/\\lambda_{k}|$.\n","On peut améliorer la convergence avec quelques astuce comme des shifts (en considérant $A-\\mu I_n$) ou des déflations (si $A$ est triangulaire par blocs).\n","## Cas des matrices symétriques\n\n","Dans le cas où $A$ est une matrice symétrique, la forme de Hessenberg $B$ de A donnée par le [théorème ci-dessus](#thm:hessenberg) est tridiagonale (et symétrique).\n","Notons pour $k =1, \\dots, n$\n","\n$$\nB_k = \\begin{pmatrix}\nb_1 & c_1 & 0  \\\\\nc_1 & b_2 & c_2 & \\\\\n0 &   c_2 & \\ddots & \\ddots \\\\\n & &   \\ddots & \\ddots & c_{k-2} & 0 \\\\\n& & & c_{k-2} & b_{k-1} & c_{k-1} \\\\\n& &  & 0 & c_{k-1} & b_k\n\\end{pmatrix},\n$$\n","de sorte que $B_n = B$.\n","On peut supposer que les $c_i$ sont tous non nuls (sinon on se ramène à un bloc).\n","On note $p_k = \\chi_{B_k}$ le polynôme caractéristique de $B_k$, et $p_0 = 1$.\n","*Proposition 16*\\\n","On a $p_1(X) = b_1-X$ et pour $k = 2, \\dots, n$,\n\n","\n$$\np_k(X) = (b_k - X)p_{k-1}(X) - c_{k-1}^2 p_{k-2}(X).\n$$\n\n","$p_k$ est scindé à racines simple sur\n","${\\mathbb{R}}$, et ses racines sont séparées par celle de\n","$p_{k-1}$.\n","La relation de récurrence est obtenue en développant le déterminant par rapport à la dernière ligne.\n\n","On voit que $p_k$ a pour terme dominant\n","$(-1)^k X^k$, de sorte que $p_k \\to +\\infty$ en\n","$-\\infty$.\n\n","Pour les racines, on raisonne par récurrence forte sur $k$. Si\n","$\\lambda$ est une racine de $p_{k-1}$, alors\n","$p_{k}(\\lambda) = - c_{k-1}^2 p_{k-2}(\\lambda)$.\n","$p_{k}(\\lambda)$ et $p_{k-1}(\\lambda)$ sont des\n","signes opposés, non nuls.\n\n","Si $\\lambda_{0,k-1}$ est la plus petite racine de\n","$p_{k-1}$, alors $p_{k-2} >0$ sur\n","$]-\\infty, \\lambda_{0,k-1}$] par hypothèse de récurrence, donc\n","$p_{k}(\\lambda_{0,k-1}) <0$ et $p_k$ admet une\n","racine $\\lambda_{0,k} < \\lambda_{0,k-1}$. Puis on construit de\n","proche en proche les racines de $p_k$ en vérifiant\n","l’entrelacement.\n","On définit $s(k,\\mu)$: c’est le signe de $p_k(\\mu)$ si ce nombre est non nul, et celui de $p_{k-1}(\\mu)$ sinon; et $\\mathscr{N}(k,\\mu)$ est le nombre de changements de signe dans\n","\n$$\n(+, s(1,\\mu), s(2,\\mu), \\dots, s(k,\\mu)).\n$$\n","*Corollaire 17*\\\n","$\\mathscr{N}(k,\\mu)$ est le nombre de racines de $p_k$ inférieures strictement à $\\mu$.\n","Soit $k \\in \\left[\\!\\!\\left[ 1, n \\right]\\!\\!\\right]$. On déduit de ce qui précède le calcul de la $k$ème valeur propre $\\lambda_k$ de $A$ par dichotomie.\n","*Algorithme:*\\\n","Soit $\\varepsilon>0$ et $[a_0,b_0$] un intervalle\n","contenant $\\lambda_k$.\n\n","Tant que $|b_i-a_i| > \\varepsilon$\n\n","$c_i = \\displaystyle\\frac{a_i+b_i}{2}$,\n\n","Si $\\mathscr{N}(n,c_i) \\geqslant k$ alors\n","$\\lambda_k \\in [a_i,c_i]$, on pose $a_{i+1} = a_i$,\n","$b_{i+1} = c_{i}$.\n\n","Sinon $\\lambda_k \\in [c_i,b_i]$, on pose $a_{i+1} = c_i$, $b_{i+1} = b_{i}$.\n"],"metadata":{}}],"metadata":{"language_info":{"name":"python","version":"3.9.1"},"kernelspec":{"name":"python3","language":"python"}},"nbformat":4,"nbformat_minor":4}