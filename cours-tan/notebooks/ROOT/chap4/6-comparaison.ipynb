{"cells":[{"cell_type":"markdown","source":["# Comparaison des méthodes\n\n","## Comparaison des méthodes itératives\n\n","*Exemple élémentaire*\\\n","Considérons le système linéaire suivant :\n\n","\n$$\n\\label{eq:example}\n\\left\\{\n\\begin{array}{ll}\n2 x_1 + x_2 &=1\\\\\nx_1 + 3x_2 &=0\n\\end{array}\n\\right.\n$$\n\n","dont la matrice est $A=\\begin{pmatrix} 2 & 1\\\\ 1 & 3\\end{pmatrix}$ et elle est symétrique définie positive. La solution du système est $x_1=3/5$, $x_2=-1/5$.\n\n","On veut approcher la solution par une méthode itérative en partant de\n\n","\n$$\n\\mathbf{x}^{(0)}=\\begin{pmatrix} {x}_1^{(0)}\\\\{x}_2^{(0)} \\end{pmatrix}=\n\\begin{pmatrix} 1\\\\\\frac{1}{2} \\end{pmatrix}.\n$$\n\n","On décrit le premier pas pour les méthodes de Jacobi, Gauss-Seidel, SOR et la méthode du gradient.\n\n","* **Méthode de Jacobi**\\\nOn a\n\n  \n$$\n\\left\\{\n\\begin{array}{ll}\nx^{(1)}_1 &=\\frac{1}{2}(1-x^{(0)}_2)\\\\\nx^{(1)}_2 &=-\\frac{1}{3}(x^{(0)}_1)\n\\end{array}\n\\right.\n\\Rightarrow\n\\left\\{\n\\begin{array}{ll}\nx^{(1)}_1 &=\\frac{1}{4}\\\\\nx^{(1)}_2 &=-\\frac{1}{3}\n\\end{array}\\right.\n$$\n\n\n* **Méthode de Gauss-Seidel**\\\nOn a\n\n  \n$$\n\\left\\{\n\\begin{array}{ll}\nx^{(1)}_1 &=\\frac{1}{2}(1-x^{(0)}_2)\\\\\nx^{(1)}_2 &=-\\frac{1}{3}x^{(1)}_1\n\\end{array}\\right.\n\\Rightarrow\n\\left\\{\n\\begin{array}{ll}\nx^{(1)}_1 &=\\frac{1}{4}\\\\\nx^{(1)}_2 &=-\\frac{1}{12}\n\\end{array}\\right.\n$$\n\n\n* **Méthode SOR**\\\nOn détermine d&#8217;abord $\\omega_{opt}$, d&#8217;après la formule [donnant le paramètre de relaxation optimal](3-splitting.ipynb#thm:omegaopt): la matrice d&#8217;itération de la méthode de Jacobi est\n\n  \n$$\nB_J=\\begin{pmatrix}0& \\frac{1}{2}\\\\\n                   -\\frac{1}{3}&0\\end{pmatrix}\n$$\n\n  On peut facilement calculer $[\\rho(B_J)^2=\\frac{1}{6}$ et donc $\\omega_{opt}=2(6-\\sqrt{30})$ ($\\approx 1.0455$).\n\n  On a donc\n\n  \n$$\n\\left\\{\n\\begin{array}{ll}\nx^{(1)}_1 &=\\frac{\\omega_{opt}}{2}(1-x^{(0)}_2)+(1-\\omega_{opt})x_1^{(0)}\\\\\nx^{(1)}_2 &=-\\frac{\\omega_{opt}}{3} x^{(1)}_1+(1-\\omega_{opt})x_2^{(0)}\n\\end{array}\\right.\n$$\n\n  \n$$\n\\Rightarrow\n\\left\\{\n\\begin{array}{ll}\nx^{(1)}_1 &=\\frac{\\omega_{opt}}{4}+1\n-\\omega_{opt}=-8+\\frac{3}{2}\\sqrt{30}\\approx 0.216\\\\\nx^{(1)}_2 &=-\\frac{\\omega_{opt}}{3}(-8+\\frac{3}{2}\\sqrt{30})+\n\\frac{1-\\omega_{opt}}{2}\\approx\n-0.098\n\\end{array}\\right.\n$$\n\n\n* **Méthode du gradient avec $P=D$**\\\nLe premier pas dans la méthode du gradient avec $P=D=\\begin{pmatrix} 2 & 0\\\\ 0& 3\\end{pmatrix}$ s&#8217;écrit :\n\n  \n$$\n\\begin{pmatrix} 2 & 0\\\\ 0& 3\\end{pmatrix}\n\\left(\n\\begin{array}{c}\nx_1^{(1)}-1\\\\\nx_2^{(1)}-\\frac{1}{2}\n\\end{array}\n\\right)=\n\\alpha_0 \\mathbf{r}^{(0)}.\n$$\n\n  où\n\n  \n$$\n\\mathbf{r}^{(0)}=\\mathbf{b}-A\\mathbf{x}^{(0)}=\n\\begin{pmatrix} 1\\\\ 0\\end{pmatrix}\n-\\begin{pmatrix} 2 & 1\\\\ 1& 3\\end{pmatrix}\\mathbf{x}^{(0)}\n=\\begin{pmatrix} -3/2\\\\ -5/2\\end{pmatrix}.\n$$\n\n  Évidemment,\n\n  \n$$\nP^{-1}=\\begin{pmatrix} 1/2 &0\\\\ 0& 1/3\\end{pmatrix}\n$$\n\n  et donc\n\n  \n$$\n\\mathbf{z}^{(0)}=P^{-1}\\mathbf{r}^{(0)}=\\begin{pmatrix} -3/4\\\\ -5/6\\end{pmatrix}\\,\\,\\,\\mbox{ d'o{\\`u} }\\,\\,\\,\n\\alpha_0=\\frac{(\\mathbf{r}^{(0)}, \\mathbf{z}^{(0)})}\n{(A\\mathbf{z}^{(0)}, \\mathbf{z}^{(0)})}=\\frac{77}{107}.\n$$\n\n\n\n","Tracons les graphes de $\\frac{\\| \\mathbf{x}^{(k)}-\\mathbf{x}\\|}{\\| \\mathbf{x}^{(0)}-\\mathbf{x}\\|}$ pour les méthodes de Jacobi, de Gauss-Seidel, SOR (avec $\\omega_{opt}$) et du gradient.\n"],"metadata":{}},{"cell_type":"code","execution_count":0,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["import numpy as np\n","from tan.syslin.jacobi import *\n","from tan.syslin.gauss_seidel import *\n","from tan.syslin.sor import *\n","from tan.syslin.gradient import *\n","\n","A = np.array([[2, 1], [1, 3]])\n","b = np.array([1, 0])\n","x0 = np.array([1, 0.5])\n","xe = np.array([3/5, -1/5])\n","tol = 1e-6\n","maxit = 100\n","[x_jac, iter_jac, inc_jac] = jacobi1(A, b, x0, tol=tol, maxiter=maxit)\n","# assert np.allclose(x_jac, xe, rtol=1e-5)\n","[x_gs, iter_gs, inc_gs] = gauss_seidel1(A, b, x0, tol=tol, maxiter=maxit)\n","# assert np.allclose(x_gs, xe, rtol=1e-5)\n","[x_sor, iter_sor, inc_sor] = sor1(A, b, x0, omega=2 * (6 - np.sqrt(30)), tol=tol, maxiter=maxit)\n","# assert np.allclose(x_sor, xe, rtol=1e-5)\n","[x_rich, iter_rich, inc_rich] = richardson(A, b, x0, alpha=0.5, P=np.diag(np.diag(A)),tol=tol, maxiter=maxit)\n","[x_grad, iter_grad, inc_grad, res_grad] = gradient(A, b, P=np.diag(np.diag(A)), x0=x0, tol=tol, maxiter=maxit)\n","# assert np.allclose(x_grad, xe, rtol=1e-5)\n","\n","print(f\"Jacobi: x={x_jac}, {iter_jac} iterations\")\n","print(f\"Gauss-Seidel: x={x_gs}, {iter_gs} iterations\")\n","print(f\"SOR: x={x_sor}, {iter_sor} iterations\")\n","print(f\"Richardson: x={x_rich}, {iter_rich} iterations\")\n","print(f\"Gradient: x={x_grad}, {iter_grad} iterations\")\n"]},{"cell_type":"markdown","source":["À présent nous pouvons tracer avec plotly les graphes de $\\frac{\\| \\mathbf{x}^{(k)}-\\mathbf{x}\\|}{\\| \\mathbf{x}^{(0)}-\\mathbf{x}\\|}$ pour les méthodes de Jacobi, de Gauss-Seidel, SOR (avec $\\omega_{opt}$) et du gradient.\n"],"metadata":{}},{"cell_type":"code","execution_count":0,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["import numpy as np\n","import plotly.graph_objects as go\n","\n","# Create the plot with plotly\n","fig = go.Figure()\n","fig.add_trace(go.Scatter(x=np.arange(0,iter_jac+1), y=inc_jac, mode='lines+markers', name='Jacobi'))\n","fig.add_trace(go.Scatter(x=np.arange(0,iter_gs+1), y=inc_gs, mode='lines+markers', name='Gauss-Seidel'))\n","fig.add_trace(go.Scatter(x=np.arange(0,iter_sor+1), y=inc_sor, mode='lines+markers', name='SOR'))\n","fig.add_trace(go.Scatter(x=np.arange(0,iter_rich+1), y=inc_rich, mode='lines+markers', name='Richardson'))\n","fig.add_trace(go.Scatter(x=np.arange(0,iter_grad+1), y=inc_grad, mode='lines+markers', name='Gradient'))\n","fig.update_layout(xaxis_title='k', yaxis_title='Incréments',yaxis_type='log')\n","fig.show()\n"]},{"cell_type":"markdown","source":["*Exemple 2: Matrice non symétrique*\\\n","On considère maintenant le système\n\n","\n$$\n\\label{eq:extwo}\n\\left\\{\n\\begin{array}{ll}\n2 x_1 + x_2 &=1\\\\\n-x_1 + 3x_2 &=0\n\\end{array}\n\\right.\n$$\n\n","dont la solution est $x_1=3/7$, $x_2=1/7$.\n\n","La matrice $A=\\begin{pmatrix} 2 & 1\\\\ -1 & 3\\end{pmatrix}$ n&#8217;est plus symétrique mais elle est clairement à diagonale dominante stricte par lignes, donc les méthodes de Jacobi et Gauss-Seidel convergent. On approche la solution par une méthode itérative en partant de\n\n","\n$$\n\\mathbf{x}^{(0)}=\\begin{pmatrix} {x}_1^{(0)}\\\\{x}_2^{(0)} \\end{pmatrix}=\n\\begin{pmatrix} 1\\\\\\frac{1}{2} \\end{pmatrix}.\n$$\n\n","Testons l&#8217;ensemble des méthodes et tracons les graphes de $\\frac{\\| \\mathbf{x}^{(k)}-\\mathbf{x}\\|}{\\| \\mathbf{x}^{(0)}-\\mathbf{x}\\|}$ pour les méthodes de Jacobi, de Gauss-Seidel, de Richardson stationnaire avec $\\alpha=0.5$ et $P=D=\\begin{pmatrix} 2 & 0\\\\ 0& 3 \\end{pmatrix}$.\n"],"metadata":{}},{"cell_type":"code","execution_count":0,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["import numpy as np\n","from tan.syslin.jacobi import *\n","from tan.syslin.gauss_seidel import *\n","from tan.syslin.sor import *\n","from tan.syslin.gradient import *\n","\n","A = np.array([[2, 1], [-1, 3]])\n","b = np.array([1, 0])\n","x0 = np.array([1, 0.5])\n","xe = np.array([3/7, 1/7])\n","tol = 1e-6\n","maxit = 100\n","[x_jac, iter_jac, inc_jac] = jacobi1(A, b, x0, tol=tol, maxiter=maxit)\n","# assert np.allclose(x_jac, xe, rtol=1e-5)\n","[x_gs, iter_gs, inc_gs] = gauss_seidel1(A, b, x0, tol=tol, maxiter=maxit)\n","# assert np.allclose(x_gs, xe, rtol=1e-5)\n","[x_sor, iter_sor, inc_sor] = sor1(A, b, x0, omega=2 * (6 - np.sqrt(30)), tol=tol, maxiter=maxit)\n","# assert np.allclose(x_sor, xe, rtol=1e-5)\n","[x_rich, iter_rich, inc_rich] = richardson(A, b, x0, alpha=0.5, P=np.diag(np.diag(A)), tol=tol, maxiter=maxit)\n","[x_grad, iter_grad, inc_grad, res_grad] = gradient(A, b, P=np.diag(np.diag(A)), x0=x0, tol=tol, maxiter=maxit)\n","# assert np.allclose(x_grad, xe, rtol=1e-5)\n","\n","print(f\"Jacobi: x={x_jac}, {iter_jac} iterations\")\n","print(f\"Gauss-Seidel: x={x_gs}, {iter_gs} iterations\")\n","print(f\"SOR: x={x_sor}, {iter_sor} iterations\")\n","print(f\"Richardson: x={x_rich}, {iter_rich} iterations\")\n","print(f\"Gradient: x={x_grad}, {iter_grad} iterations\")\n"]},{"cell_type":"markdown","source":["Tracons en échelle semi-logarithmique les graphes de $\\| \\mathbf{x}^{(k+1)}-\\mathbf{x}^{(k)}\\|$ pour les méthodes de Jacobi, de Gauss-Seidel, de Richardson stationnaire avec $\\alpha=0.5$ et $P=D=\\begin{pmatrix} 2 & 0\\\\ 0& 3 \\end{pmatrix}$.\n"],"metadata":{}},{"cell_type":"code","execution_count":0,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["import numpy as np\n","import plotly.graph_objects as go\n","\n","fig = go.Figure()\n","fig.add_trace(go.Scatter(x=np.arange(0,iter_jac+1), y=inc_jac, mode='lines+markers', name='Jacobi'))\n","fig.add_trace(go.Scatter(x=np.arange(0,iter_gs+1), y=inc_gs, mode='lines+markers', name='Gauss-Seidel'))\n","fig.add_trace(go.Scatter(x=np.arange(0,iter_sor+1), y=inc_sor, mode='lines+markers', name='SOR'))\n","fig.add_trace(go.Scatter(x=np.arange(0,iter_rich+1), y=inc_rich, mode='lines+markers', name='Richardson'))\n","fig.add_trace(go.Scatter(x=np.arange(0,iter_grad+1), y=inc_grad, mode='lines+markers', name='Gradient'))\n","fig.update_layout(xaxis_title='k', yaxis_title='Incréments',yaxis_type='log')\n","fig.show()\n"]},{"cell_type":"markdown","source":["*Note:* cette fois-ci, puisque la matrice $A$ n&#8217;est plus symétrique définie positive, on n&#8217;a aucune garantie que la méthode du gradient converge. En effet, même si elle converge, la vitesse de convergence peut être assez lente car elle dépend du choix de $\\alpha$.\n","## Convergence\n\n","On donne des exemples de convergence de méthodes itératives appliquées à quelques systèmes linéaires $A\\mathbf{x}=\\mathbf{b}$.\n","## Matrice mal conditionnée\n\n","*Exemple 3*\\\n","On se donne la matrice\n\n","\n$$\n\\label{eq:ex3_met_it}\nA=\\begin{pmatrix}\n5&7\\\\7 & 10\n\\end{pmatrix}\n$$\n\n","La matrice est mal conditionnée ($K_2(A)\\approx 223$).\n\n","On considère la méthode de Gauss-Seidel et la méthode du gradient statique avec $\\alpha=0.5$ et $P=\\operatorname{diag}(A)$.\n\n","On a $\\rho(B_{GS})=0.995$ et $\\rho(B_{Grad})=0.98$, où $B_{Grad}= I-\\alpha P^{-1}A$ est la matrice d&#8217;itération pour la méthode du gradient.\n\n","voici le code pour calculer les solutions\n"],"metadata":{"node_name":"admonition"}},{"cell_type":"code","execution_count":0,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["import numpy as np\n","from tan.syslin.jacobi import *\n","from tan.syslin.gauss_seidel import *\n","from tan.syslin.sor import *\n","from tan.syslin.gradient import *\n","\n","A = np.array([[5, 7], [7, 10]])\n","b = np.array([1, 0])\n","x0 = np.array([1, 0.5])\n","\n","tol = 1e-6\n","maxit = 1000\n","\n","[x_gs, iter_gs, inc_gs] = gauss_seidel1(A, b, x0, tol=tol, maxiter=maxit)\n","[x_grad, iter_grad, inc_grad, res_grad] = gradient(A, b, P=np.diag(np.diag(A)), x0=x0, tol=tol, maxiter=maxit)\n","\n","print(f\"Gauss-Seidel: x={x_gs}, {iter_gs} iterations\")\n","print(f\"Gradient: x={x_grad}, {iter_grad} iterations\")\n"]},{"cell_type":"markdown","source":["La figure ci-dessous montre le comportement de l&#8217;erreur relative pour les deux méthodes.\n"],"metadata":{}},{"cell_type":"code","execution_count":0,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["import numpy as np\n","import plotly.graph_objects as go\n","\n","fig = go.Figure()\n","fig.add_trace(go.Scatter(x=np.arange(0,iter_gs+1), y=inc_gs, mode='lines+markers', name='Gauss-Seidel'))\n","fig.add_trace(go.Scatter(x=np.arange(0,iter_grad+1), y=inc_grad, mode='lines+markers', name='Gradient'))\n","fig.update_layout(xaxis_title='k', yaxis_title='Incréments',yaxis_type='log')\n","fig.show()\n"]},{"cell_type":"markdown","source":["*Note:* On peut comparer le résultat précédent avec les mêmes courbes pour une matrice $2\\times 2$ avec un bon conditionnement ($K(A)\\approx\n2.6$).\n","## La corde élastique\n\n","*La corde élastique*\\\n","On reprend l&#8217;exemple du *fil élastique* et on résout le système pour $n=25$ avec les méthodes de Gauss-Seidel, du gradient avec $P=\\operatorname{diag}(A)$ et du gradient conjugué préconditionné ($P=\\operatorname{diag}(A)$), en partant du vecteur initial $\\mathbf{x}^{(0)} = \\mathbf{0}$ et avec une tolérance de $10^{-6}$ (sur le résidu relatif).\n\n","Voilà le code Python  :\n"],"metadata":{"node_name":"admonition"}},{"cell_type":"code","execution_count":0,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["import numpy as np\n","from tan.syslin.jacobi import *\n","from tan.syslin.gauss_seidel import *\n","from tan.syslin.sor import *\n","from tan.syslin.gradient import *\n","from tan.syslin.pcg import *\n","\n","n = 25\n","h = 1 / (n + 1)\n","\n","# Creating matrix A\n","main_diag = (2 / h) * np.ones(n)\n","off_diag = (-1 / h) * np.ones(n - 1)\n","A = np.diag(main_diag) + np.diag(off_diag, 1) + np.diag(off_diag, -1)\n","b = h * np.ones(n)\n","\n","print(f\"Conditionnement de A: {np.linalg.cond(A)}\")\n","\n","x0 = np.zeros(n)\n","tol = 1e-6\n","maxit=10000\n","\n","[x_jac, iter_jac, inc_jac] = jacobi1(A, b, x0, tol=tol, maxiter=maxit)\n","[x_gs, iter_gs, inc_gs] = gauss_seidel1(A, b, x0, tol=tol, maxiter=maxit)\n","[x_sor, iter_sor, inc_sor] = sor1(A, b, x0, omega=2 * (6 - np.sqrt(30)), tol=tol, maxiter=maxit)\n","[x_rich, iter_rich, inc_rich] = richardson(A, b, x0, alpha=0.5, P=np.diag(np.diag(A)), tol=tol, maxiter=maxit)\n","[x_grad, iter_grad, inc_grad, res_grad] = gradient(A, b, P=np.diag(np.diag(A)), x0=x0, tol=tol, maxiter=maxit)\n","[x_cg, iter_cg, inc_cg, res_cg] = pcg(A, b, P=np.diag(np.diag(A)), x0=x0, tol=tol, maxiter=maxit)\n","\n","print(f\"Jacobi: x={x_jac}, {iter_jac} iterations\")\n","print(f\"Gauss-Seidel: x={x_gs}, {iter_gs} iterations\")\n","print(f\"SOR: x={x_sor}, {iter_sor} iterations\")\n","print(f\"Richardson: x={x_rich}, {iter_rich} iterations\")\n","print(f\"Gradient: x={x_grad}, {iter_grad} iterations\")\n","print(f\"CG: x={x_cg}, {iter_cg} iterations\")\n"]},{"cell_type":"markdown","source":["Tracons les graphes de $\\frac{\\| \\mathbf{x}^{(k)}-\\mathbf{x}\\|}{\\| \\mathbf{x}^{(0)}-\\mathbf{x}\\|}$ pour les méthodes de Jacobi, de Gauss-Seidel, SOR (avec $\\omega_{opt}$), du gradient stationnaire (Richardson), du gradient dynamique et du gradient conjugué préconditionné.\n"],"metadata":{}},{"cell_type":"code","execution_count":0,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["import numpy as np\n","import plotly.graph_objects as go\n","\n","fig = go.Figure()\n","fig.add_trace(go.Scatter(x=np.arange(0,iter_jac+1), y=inc_jac, mode='lines+markers', name='Jacobi'))\n","fig.add_trace(go.Scatter(x=np.arange(0,iter_gs+1), y=inc_gs, mode='lines+markers', name='Gauss-Seidel'))\n","fig.add_trace(go.Scatter(x=np.arange(0,iter_sor+1), y=inc_sor, mode='lines+markers', name='SOR'))\n","fig.add_trace(go.Scatter(x=np.arange(0,iter_rich+1), y=inc_rich, mode='lines+markers', name='Richardson'))\n","fig.add_trace(go.Scatter(x=np.arange(0,iter_grad+1), y=inc_grad, mode='lines+markers', name='Gradient'))\n","fig.add_trace(go.Scatter(x=np.arange(0,iter_cg+1), y=inc_cg, mode='lines+markers', name='CG'))\n","fig.update_layout(xaxis_title='k', yaxis_title='Incréments',yaxis_type='log')\n","fig.show()\n"]},{"cell_type":"markdown","source":["## Matrice de Hilbert\n\n","*Matrice de Hilbert*\\\n","On prend l&#8217;exemple de la matrice de Hilbert $A=\\{a_{ij}\\}_{1\\leq i,j\\leq n}$,\n\n","\n$$\na_{ij}=\\frac{1}{i+j-1},\\quad 1\\leq i,j\\leq n\n$$\n\n","et on résout le système linéaire pour tout $n$ par la méthode itérative du gradient avec un préconditionneur diagonale, tolérance de $10^{-6}$ sur le résidu relatif et $\\mathbf{x}_0 = \\boldsymbol{0}$.\n\n","On évalue l&#8217;erreur relative entre la solution calculée et la solution exacte. Le résultat a déjà été montré en figure #fig1:ex-luca2[1.2].\n\n","Voici le code que nous pouvons utiliser\n"],"metadata":{}},{"cell_type":"code","execution_count":0,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["import numpy as np\n","from scipy.linalg import hilbert\n","import plotly.graph_objects as go\n","import pandas as pd\n","from tan.syslin.gradient import *\n","from tan.syslin.pcg import *\n","\n","nn = []\n","Acond = []\n","relres_gr = []\n","iter_gr = []\n","erreur_gr = []\n","relres_pcg = []\n","iter_pcg = []\n","erreur_pcg = []\n","\n","for j in range(2, 8):\n","    n = 2 * j\n","    i = j - 1\n","    nn.append(n)\n","\n","    A = hilbert(n)\n","    x_ex = np.ones((n))\n","    b = A @ x_ex\n","    Acond.append(np.linalg.cond(A))\n","    P = np.diag(np.diag(A))\n","    x0 = np.zeros((n))\n","    x, iterations, inc, res = gradient(A, b, P=P, x0=x0, tol=1.e-14, maxiter=10000)\n","    x_cg, iterations_cg, inc_cg, res_cg = pcg(A, b, P=P, x0=x0, tol=1.e-14, maxiter=10000)\n","    iter_gr.append(iterations)\n","    iter_pcg.append(iterations_cg)\n","\n","    erreur_gr.append(np.linalg.norm(x - x_ex) / np.linalg.norm(x_ex))\n","    erreur_pcg.append(np.linalg.norm(x_cg - x_ex) / np.linalg.norm(x_ex))\n","\n","# dataframe\n","df = pd.DataFrame({\n","    'nn': nn,\n","    'Acond': Acond,\n","    'iter_gr': iter_gr,\n","    'erreur_gr': erreur_gr,\n","    'iter_pcg': iter_pcg,\n","    'erreur_pcg': erreur_pcg,\n","})\n","\n","# affiche la table\n","print(df.to_markdown())\n"]},{"cell_type":"markdown","source":["Nous pouvons à présent tracer les graphes de $K(A)$ et $\\frac{\\| \\mathbf{x}^{(k)}-\\mathbf{x}\\|}{\\| \\mathbf{x}^{(0)}-\\mathbf{x}\\|}$ pour la méthode du gradient  avec $P=\\operatorname{diag}(A)$.\n"],"metadata":{}},{"cell_type":"code","execution_count":0,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["# Create the plot with plotly\n","fig = go.Figure()\n","\n","fig.add_trace(go.Scatter(x=nn, y=Acond, mode='lines', name='Acond'))\n","fig.add_trace(go.Scatter(x=nn, y=erreur_gr, mode='lines+markers', line=dict(dash='dot'), name='erreur_gr'))\n","fig.add_trace(go.Scatter(x=nn, y=erreur_pcg, mode='lines+markers', line=dict(dash='dot'), name='erreur_pcg'))\n","\n","fig.update_layout(yaxis_type=\"log\")\n","\n","fig.show()\n","\n"]},{"cell_type":"markdown","source":["Le tableau ci-dessous résume les erreurs obtenues en utilisant la factorisation $LU$ et la méthode itérative.\n\n","\n","|  |  | $LU$ | Méthode du gradient |  |  |\n","|  |  | ---- | ------------------- |  |  |\n","| $n$ | $K(A)$ | Erreur | Erreur | Itérations | Résidu |\n","| 4 | 1.55e+04 | 1.94e-13 | 8.72e-03 | 995 | 1.00e-06 |\n","| 6 | 1.50e+07 | 1.18e-10 | 3.60e-03 | 1813 | 9.99e-07 |\n","| 8 | 1.53e+10 | 5.23e-08 | 6.30e-03 | 1089 | 9.96e-07 |\n","| 10 | 1.60e+13 | 2.38e-04 | 7.99e-03 | 875 | 9.99e-07 |\n","| 12 | 1.67e+16 | 1.41e-01 | 5.09e-03 | 1355 | 9.99e-07 |\n","| 14 | 2.04e+17 | 9.82e-01 | 3.91e-03 | 1379 | 9.98e-07 |\n","\n","## Déplacement d&#8217;une structure pleine soumise à des forces\n\n","*Déplacement d&#8217;une structure pleine soumise à des forces*\\\n","On a pris une matrice qui dérive d&#8217;un problème de calcul du déplacement d&#8217;une structure pleine soumise à des forces données, discrétisé par la méthode des éléments finis. On a considéré des discrétisations de plus en plus fines, ce qui nous a donné des matrices de taille croissante.\n\n","On a résolu les systèmes linéaires ainsi obtenus par la factorisation de Cholesky et par la méthode du gradient conjugué, avec un bon préconditionneur (*à savoir* un préconditionneur qui utilise une factorisation de Cholesky incomplète).\n\n","Le tableau ci-dessous montre dans les deux premières colonnes la taille $n$ de la matrice ainsi que sa *sparsité*, c-à-d. le rapport entre les éléments non nuls $m$ et le nombre total d&#8217;éléments $n^2$. Dans les colonnes suivantes on montre le nombre d&#8217;opérations effectuées et l&#8217;emplacement mémoire utilisé par les deux méthodes. On remarque que pour des matrices de taille considérable (on rappelle qu&#8217;il s&#8217;agit de matrices creuses!) la méthode itérative est avantageuse par rapport à la factorisation de Cholesky.\n\n","\n","|  |  | Cholesky |  | Gradient Conjugué |  | flops(Chol.)/ | Mem(Chol.)/ |\n","|  |  | -------- |  | ----------------- |  | ------------- | ----------- |\n","| 1-6 $n$ | $m/n^2$ | flops | Memoire | flops | Memoire | flops(GC) | Mem(GC) |\n","| 47 | 0.12 | 8.05e+03 | 464 | 1.26e+04 | 228 | 0.64 | 2.04 |\n","| 83 | 0.07 | 3.96e+04 | 1406 | 3.03e+04 | 533 | 1.31 | 2.64 |\n","| 150 | 0.04 | 2.01e+05 | 4235 | 8.86e+04 | 1245 | 2.26 | 3.4 |\n","| 225 | 0.03 | 6.39e+05 | 9260 | 1.95e+05 | 2073 | 3.27 | 4.47 |\n","| 329 | 0.02 | 1.74e+06 | 17974 | 3.39e+05 | 3330 | 5.15 | 5.39 |\n","| 424 | 0.02 | 3.78e+06 | 30815 | 5.49e+05 | 4513 | 6.88 | 6.83 |\n","| 530 | 0.01 | 8.31e+06 | 50785 | 8.61e+05 | 5981 | 9.65 | 8.49 |\n","| 661 | 0.01 | 1.19e+07 | 68468 | 1.11e+06 | 7421 | 10.66 | 9.23 |\n","\n"],"metadata":{}}],"metadata":{"language_info":{"name":"python","version":"3.9.1"},"kernelspec":{"name":"python3","language":"python"}},"nbformat":4,"nbformat_minor":4}