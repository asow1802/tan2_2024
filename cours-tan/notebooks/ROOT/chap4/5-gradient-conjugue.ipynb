{"cells":[{"cell_type":"markdown","source":["# La méthode du gradient conjugué\n\n","La méthode du gradient consiste essentiellement en deux phases: choisir une direction de descente (celle du résidu) et un point où $\\Phi$ atteint un minimum local le long de cette direction.\n","La seconde phase est indépendante de la première.\n","En effet, étant donné une direction $\\mathbf{p}^{(k)}$, on peut choisir $\\alpha_k$ comme étant la valeur du paramètre $\\alpha$ telle que $\\Phi(\\mathbf{x}^{(k)} + \\alpha \\mathbf{p}^{(k)})$ est minimum.\n","En écrivant que la dérivée par rapport à $\\alpha$ s&#8217;annule au point où la fonction admet un minimum local, on obtient\n"],"metadata":{}},{"cell_type":"markdown","source":["\n$$\n\\label{alphap}\n\\alpha_k = \\frac{ ({\\mathbf{p}^{(k)}},\\mathbf{r}^{(k)} )}{ ({\\mathbf{p}^{(k)}}, A \\mathbf{p}^{(k)} )}\n$$\n"],"metadata":{}},{"cell_type":"markdown","source":["*Note:* La question est de savoir comment choisir $\\mathbf{p}^{(k)}$.\n"],"metadata":{"node_name":"admonition"}},{"cell_type":"markdown","source":["Une ***alternative*** à l&#8217;approche qui nous a conduit à identifier $\\mathbf{p}^{(k)}$ avec $\\mathbf{r}^{(k)}$ est proposée dans la suite.\n"],"metadata":{}},{"cell_type":"markdown","source":["*Définition: Vecteur optimal*\\\n","Un vecteur $\\mathbf{x}^{(k)}$ est dit *optimal* par rapport à une direction $\\mathbf{p} \\neq \\mathbf{0}$ si\n\n","\n$$\n\\label{star2}\n\\Phi ( {\\bf x}^{(k)} ) \\leq \\Phi ( {\\bf x}^{(k)} + \\lambda {\\bf p} ), \\qquad\n\\forall \\lambda \\in \\mathbb{R}\n$$\n"],"metadata":{}},{"cell_type":"markdown","source":["Si $\\mathbf{x}^{(k)}$ est optimal par rapport à n&#8217;importe quelle direction d&#8217;un espace vectoriel $V$, on dit que $\\mathbf{x}^{(k)}$ est optimal par rapport à $V$.\n"],"metadata":{}},{"cell_type":"markdown","source":["Il découle de la définition de l&#8217;optimalité que $\\mathbf{p}$ doit être orthogonal au résidu $\\mathbf{r}^{(k)}$. En effet, on déduit de [[eq18]](#eq18) que $\\Phi$ admet un minimum local le long de $\\mathbf{p}$ pour $\\lambda=0$. La dérivée de $\\Phi$ par rapport à $\\lambda$ doit donc s&#8217;annuler pour $\\lambda=0$. Or\n"],"metadata":{}},{"cell_type":"markdown","source":["\n$$\n\\frac{d}{d \\lambda} \\Phi( \\mathbf{x}^{(k)} + \\lambda \\mathbf{p} ) =\n(\\mathbf{p}, A \\mathbf{x}^{(k)} - \\mathbf{b}) + \\lambda (\\mathbf{p},A\\mathbf{p})\n$$\n"],"metadata":{}},{"cell_type":"markdown","source":["On a donc\n"],"metadata":{}},{"cell_type":"markdown","source":["\n$$\n\\left.\\frac{d}{d \\lambda} \\Phi(  \\mathbf{x}^{(k)} + \\lambda \\mathbf{p})\n\\right|_{\\lambda = 0} = 0 \\quad \\Longleftrightarrow \\quad (\\mathbf{p},\\mathbf{r}^{(k)}) = 0\n$$\n"],"metadata":{}},{"cell_type":"markdown","source":["c&#8217;est-à-dire, $\\mathbf{p} \\perp \\mathbf{r}^{(k)}$. Remarquer que le vecteur $\\mathbf{x}^{(k+1)}$ de la méthode du gradient (non préconditionné) est optimal par rapport à $\\mathbf{r}^{(k)}$ puisque, d&#8217;après le choix de $\\alpha_k$, on a $\\mathbf{r}^{(k+1)} \\perp \\mathbf{r}^{(k)}$, mais cette propriété n&#8217;est plus vraie pour l&#8217;itérée suivante $\\mathbf{x}^{(k+2)}$. On est donc amené à se demander s&#8217;il existe des directions de descente qui conservent l&#8217;optimalité des vecteurs à chaque itération. Soit donc\n"],"metadata":{}},{"cell_type":"markdown","source":["\n$$\n\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + \\mathbf{q}\n$$\n"],"metadata":{}},{"cell_type":"markdown","source":["et supposons $\\mathbf{x}^{(k)}$ optimal par rapport à une direction $\\mathbf{p}$ (ainsi $\\mathbf{r}^{(k)} \\perp \\mathbf{p}$). Imposons que $\\mathbf{x}^{(k+1)}$ demeure optimal par rapport à $\\mathbf{p}$, c&#8217;est-à-dire $\\mathbf{r}^{(k+1)}  \\perp \\mathbf{p}$. On obtient alors\n"],"metadata":{}},{"cell_type":"markdown","source":["\n$$\n0 = (\\mathbf{p},\\mathbf{r}^{(k+1)}) = (\\mathbf{p},\\mathbf{r}^{(k)} - A\\mathbf{q} ) =\n- (\\mathbf{p},A\\mathbf{q})\n$$\n"],"metadata":{}},{"cell_type":"markdown","source":["Ainsi, pour préserver l&#8217;optimalité entre les itérées successives, les directions de descentes doivent être mutuellement *A-orthogonales* (on dit aussi *A-conjuguées*), *i.e.*\n"],"metadata":{}},{"cell_type":"markdown","source":["\n$$\n(\\mathbf{p},A\\mathbf{q}) = 0\n$$\n"],"metadata":{}},{"cell_type":"markdown","source":["Une méthode qui utilise des directions de descente A-conjuguées est dite *conjuguée*. Expliquons maintenant comment générer automatiquement une suite de directions conjuguées. Soit $\\mathbf{p}^{(0)} = \\mathbf{r}^{(0)}$ et cherchons une direction de la forme\n"],"metadata":{}},{"cell_type":"markdown","source":["\n$$\n\\label{zerocg}\n    \\mathbf{p}^{(k+1)} = \\mathbf{r}^{(k+1)} - \\beta_k  \\mathbf{p}^{(k)}, \\quad k = 0, 1, \\ldots\n$$\n"],"metadata":{}},{"cell_type":"markdown","source":["où $\\beta_k \\in \\mathbb{R}$ est à déterminer de manière à ce que\n"],"metadata":{}},{"cell_type":"markdown","source":["\n$$\n\\label{unocg}\n(A \\mathbf{p}^{(j)},\\mathbf{p}^{(k+1)}) = 0, \\quad j = 0, 1, \\ldots, k\n$$\n"],"metadata":{}},{"cell_type":"markdown","source":["En écrivant [[eq20]](#eq20) pour $j=k$, on obtient à partir de [[eq19]](#eq19)\n"],"metadata":{}},{"cell_type":"markdown","source":["\n$$\n\\label{betak}\n   \\beta_k = \\frac{( A\\mathbf{p}^{(k)}, \\mathbf{r}^{(k+1)})}\n    {( A\\mathbf{p}^{(k)},\\mathbf{p}^{(k)})}, \\quad k = 0, 1, \\ldots\n$$\n"],"metadata":{}},{"cell_type":"markdown","source":["On peut montrer (par recurrence) que l&#8217;égalité [[eq20]](#eq20) est aussi vérifiée pour $j = 0, 1, \\ldots, k-1$.\n"],"metadata":{}},{"cell_type":"markdown","source":["La méthode du gradient conjugué (notée GC) est donc la méthode obtenue en choisissant comme directions de descente les vecteurs latexmath:[{\\bf\n","  p}^{(k)}] donnés par [[eq19]](#eq19) et comme paramètres d&#8217;accélération les $\\alpha_k$ définis en [[eq17]](#eq17). Par conséquent, en posant ${\\bf r}^{(0)} = {\\bf b} - {\\rm A} {\\bf x}^{(0)}$ et ${\\bf p}^{(0)} = {\\bf r}^{(0)}$, la $k$-ième itération de la méthode du gradient conjugué s&#8217;écrit\n"],"metadata":{}},{"cell_type":"markdown","source":["\n$$\n\\begin{array}{l}\n   \\displaystyle{\\alpha_k = \\frac{ ({\\bf p}^{(k)},{\\bf r}^{(k)}) }{ ({\\bf p}^{(k)}, {\\rm A} {\\bf p}^{(k)})} }\n   \\\\[5pt]\n   {\\bf x}^{(k+1)} = {\\bf x}^{(k)} + \\alpha_k {\\bf p}^{(k)}  \\\\[5pt]\n   {\\bf r}^{(k+1)} = {\\bf r}^{(k)} - \\alpha_k {\\rm A} {\\bf p}^{(k)}  \\\\[5pt]\n   \\displaystyle{\\beta_{k}     =  \\frac{( {\\rm A}{\\bf p}^{(k)},{\\bf r}^{(k+1)} ) }\n                       { ({\\bf p}^{(k)}, {\\rm A}{\\bf p}^{(k)}) }  } \\\\[5pt]\n   {\\bf p}^{(k+1)} = {\\bf r}^{(k+1)} - \\beta_{k} {\\bf p}^{(k)}\n\\end{array}\n$$\n"],"metadata":{}},{"cell_type":"markdown","source":["Concernant la convergence de la méthode du gradient conjugué, on a le résultat suivant:\n"],"metadata":{}},{"cell_type":"markdown","source":["*Théorème: convergence de la méthode du gradient conjugué*\\\n","Soit A une matrice symétrique définie positive d&#8217;ordre $n$. Toute méthode qui utilise des directions conjuguées pour résoudre $A\\mathbf{x}=\\mathbf{b}$ conduit à la solution exacte en au plus $n$ itérations (en arithmétique exacte!).\n"],"metadata":{}},{"cell_type":"markdown","source":["*Preuve*\\\n","Les directions ${\\bf p}^{(0)}, {\\bf p}^{(1)}, \\ldots, {\\bf p}^{(n-1)}$ forment une base A-orthogonale de $\\mathbb{R}^n$. De plus, puisque ${\\bf x}^{(n)}$ est optimal par rapport à toutes les directions ${\\bf p}^{(j)}$, $j=0, \\ldots, n-1$, le vecteur ${\\bf r}^{(n)}$ est orthogonal à l&#8217;espace $S_{n-1} = \\langle {\\bf p}^{(0)}, {\\bf p}^{(1)},\\ldots, {\\bf p}^{(n-1)} \\rangle$. Par conséquent, ${\\bf r}^{(n)} \\perp S_{n-1} = \\mathbb{R}^n$ et donc ${\\bf r}^{(n)} = {\\bf 0}$ ce qui implique ${\\bf x}^{(n)} = {\\bf x}$. $\\blacksquare$\n"],"metadata":{}},{"cell_type":"markdown","source":["On démontre aussi le théorème suivant:\n"],"metadata":{}},{"cell_type":"markdown","source":["*Théorème: estimation de l&#8217;erreur de la méthode du gradient conjugué*\\\n","Soit $A$ une matrice symétrique définie positive. La méthode du gradient conjugué pour la résolution de $A\\mathbf{x}=\\mathbf{b}$ converge après au plus $n$ étapes.\n","De plus, l&#8217;erreur ${\\bf e}^{(k)}$ à la $k$-ème itération (avec $k < n$) est orthogonale à ${\\bf p}^{(j)}$, pour $j=0, \\ldots, k-1$ et\n\n","\n$$\n\\label{errcg}\n\\| {\\bf e}^{(k)} \\|_{\\rm A} \\leq\n\\frac{2 c^k}{1+c^{2k}} \\| {\\bf e}^{(0)} \\|_{\\rm A}, \\quad \\mbox{avec }\nc = \\frac{\\sqrt{K_2 ({\\rm A}) } - 1 } {\\sqrt{K_2 ( {\\rm A}) } + 1}.\n$$\n"],"metadata":{}},{"cell_type":"markdown","source":["La $k$-ième itération de la méthode du gradient conjugué n&#8217;est bien définie que si la direction de descente ${\\bf p}^{(k)}$ est non nulle. En outre, si ${\\bf p}^{(k)} = {\\bf 0}$, alors l&#8217;itérée ${\\bf  x}^{(k)}$ doit coı̈ncider avec la solution **x** du système.\n"],"metadata":{}},{"cell_type":"markdown","source":["*Note:* En l&#8217;absence d&#8217;erreur d&#8217;arrondi, la méthode du gradient conjugué peut donc être vue comme une méthode directe puisqu&#8217;elle converge en un nombre fini d&#8217;étapes Néanmoins, pour les matrices de grandes tailles, elle est généralement utilisée comme méthode itérative puisqu&#8217;on l&#8217;interrompt dès que l&#8217;erreur devient inférieure à une tolérance fixée.\nDe ce point de vue, la dépendance de l&#8217;erreur par rapport au conditionnement de la matrice est plus favorable que pour la méthode du gradient.\nSignalons également que l&#8217;estimation [[eq22]](#eq22) est souvent beaucoup trop pessimiste et ne prend pas en compte le fait que dans cette méthode, et contrairement à la méthode du gradient, la convergence est influencée par la *totalité* du spectre de A, et pas seulement par les valeurs propres extrémales.\n"],"metadata":{"node_name":"admonition"}},{"cell_type":"markdown","source":["*Effet des erreurs d&#8217;arrondi*\\\n","La méthode du gradient conjugué ne converge en un nombre fini d&#8217;étapes qu&#8217;en arithmétique exacte. L&#8217;accumulation des erreurs d&#8217;arrondi détruit l&#8217;A-orthogonalité des directions de descente et peut même provoquer des divisions par zéro lors du calcul des coefficients $\\alpha_k$ et $\\beta_k$. Ce dernier phénomène (appelé *breakdown* dans la littérature anglo-saxonne) peut être évité grâce à des procédés de stabilisation; on parle alors de méthodes de gradient stabilisées. Il se peut, malgré ces stratégies, que GC ne converge pas (en arithmétique finie) après $n$ itérations. Dans ce cas, la seule possibilité raisonnable est de redémarrer les itérations avec le dernier résidu calculé. On obtient alors la méthode du *gradient conjugué cyclique* ou *méthode du gradient conjugué avec redémarrage*, qui ne possède pas les propriétés de convergence de GC.\n\n","## La méthode du gradient conjugué préconditionné (Sec. 4.3.5 du livre)\n\n","Si $P$ est une matrice de préconditionnement symétrique définie positive, la méthode du gradient conjugué préconditionné consiste à appliquer la méthode du gradient conjugué au système préconditionné\n","\n$$\n\\begin{aligned}\n{\\rm P}^{-1/2} {\\rm A} {\\rm P}^{-1/2} {\\bf y} = {\\rm P}^{-1/2} {\\bf b},\n\\qquad \\mbox{ avec } {\\bf y} = {\\rm P}^{1/2} {\\bf x}\n\\end{aligned}\n$$\n","En pratique, la méthode est implémentée sans calculer explicitement $P^{1/2}$ ou $P^{-1/2}$.\n","Après un peu d&#8217;algèbre, on obtient le schéma suivant:\n","on se donne ${\\bf x}^{(0)}$ et on pose ${\\bf r}^{(0)} = {\\bf b} - {\\rm A}{\\bf x}^{(0)}$, latexmath:[{\\bf z}^{(0)} =\n","{\\rm P}^{-1} {\\bf r}^{(0)}] et ${\\bf p}^{(0)} = {\\bf z}^{(0)}$, la $k$-ième itération s&#8217;écrit alors\n","\n$$\n\\begin{array}{l}\n   \\displaystyle{\\alpha_k =\n \\frac{ ({\\bf p}^{(k)}, {\\bf r}^{(k)}) }{ ({\\bf p}^{(k)},{\\rm A} {\\bf\n    p}^{(k)})}}\\\\[5pt]\n   {\\bf x}^{(k+1)} = {\\bf x}^{(k)} + \\alpha_k {\\bf p}^{(k)}\n   \\\\[5pt]\n   {\\bf r}^{(k+1)} = {\\bf r}^{(k)} - \\alpha_k {\\rm A} {\\bf p}^{(k)}\n   \\\\[5pt]\n   {\\rm P} {\\bf z}^{(k+1)} = {\\bf r}^{(k+1)} \\\\[5pt]\n   \\displaystyle{\\beta_{k}     =  \\frac{ ( {\\rm A}{\\bf p}^{(k)}, {\\bf z}^{(k+1)})  }\n                       { ({\\bf p}^{(k)}, {\\rm A}{\\bf p}^{(k)}) }\n                 }      \\\\[5pt]\n   {\\bf p}^{(k+1)} = {\\bf z}^{(k+1)} - \\beta_{k} {\\bf p}^{(k)}\n \\end{array}\n$$\n","*Note:* Le coût du calcul est plus élevé que pour GC puisqu&#8217;on doit résoudre à chaque itération le système linéaire ${\\rm P} {\\bf z}^{(k+1)}\n= {\\bf r}^{(k+1)}$. L&#8217;estimation de l&#8217;erreur est la même que pour la méthode non préconditionnée, à condition de remplacer A par ${\\rm P}^{-1} {\\rm A}$.\n","## Implémentation\n\n","Voici une implémentation de la méthode du gradient conjugué préconditionné pour la résolution d&#8217;un système linéaire $A\\mathbf{x}=\\mathbf{b}$ où $A$ est une matrice symétrique définie positive et $P$ est une matrice de préconditionnement symétrique définie positive.\n"],"metadata":{}},{"cell_type":"code","execution_count":0,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["Unresolved directive in 5-gradient-conjugue.adoc - include::example$tan/syslin/pcg.py[]\n"]},{"cell_type":"markdown","source":["## Exemple\n\n","Reprenons la matrice symétrique définie positive qui nous a servie à illustrer l&#8217;algorithme du gradient.\n"],"metadata":{}},{"cell_type":"code","execution_count":0,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["import numpy as np\n","# pick a 4x4 matrix symétrique définie positive\n","A=np.array([[2.,-1.,0.,0.],[-1.,2.,-1.,0.],[0.,-1.,2.,-1.],[0.,0.,-1.,2.]])\n","print(f\"A = {A}\")\n","# on calcule les valeurs propres de A\n","print(f\"Valeurs propres de A: {np.linalg.eigvals(A)}\")\n","print(f\"Conditionnement de A: {np.linalg.cond(A)}\")\n"]},{"cell_type":"markdown","source":["On peut maintenant résoudre le système $A\\mathbf{x}=\\mathbf{b}$ avec la méthode du gradient conjugué préconditionné.\n"],"metadata":{}},{"cell_type":"code","execution_count":0,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["from tan.syslin.pcg import pcg\n","[x_pcg,niter_pcg,res_pcg,inc_pcg]=pcg( A, np.array([1.,0.,1.,0]), P=np.diag(np.diag(A)) )\n","print(f\"Solution: x = {x_pcg}\")\n","print(f\"Nombre d'itérations: {niter_pcg}\")\n"]},{"cell_type":"markdown","source":["Nous pouvons tracer la convergence du résidu ainsi que celle de l&#8217;incrément avec `Plotly`\n"],"metadata":{}},{"cell_type":"code","execution_count":0,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["import plotly.graph_objects as go\n","fig = go.Figure()\n","fig.add_trace(go.Scatter(x=np.arange(0,niter_pcg+1), y=inc_pcg, mode='lines+markers', name='Incrément'))\n","fig.add_trace(go.Scatter(x=np.arange(0,niter_pcg+1), y=res_pcg, mode='lines+markers', name='Résidu'))\n","fig.update_layout(title='Convergence de la méthode du gradient conjugué préconditionné', xaxis_title='Itération', yaxis_title='Incrément et Résidu', yaxis_type='log')\n","fig.show()\n"]},{"cell_type":"markdown","source":["*Note:* nous observons que le résidu et l&#8217;incrément atteignent le 0 machine en 4 itérations. Cela est dû au fait que la matrice $A$ est de taille 4 et que la méthode du gradient conjugué converge en au plus 4 itérations pour une matrice de taille 4, voir cette [remarque](#pcg_exact).\n"],"metadata":{"node_name":"admonition"}}],"metadata":{"language_info":{"name":"python","version":"3.9.1"},"kernelspec":{"name":"python3","language":"python"}},"nbformat":4,"nbformat_minor":4}