{"cells":[{"cell_type":"markdown","source":["# La méthode du gradient (ou de Richardson dynamique)\n\n","On introduit la notion de résidu à la $k$-ième itération :\n"],"metadata":{}},{"cell_type":"markdown","source":["\n$$\n\\mathbf{r}^{(k)}=\\mathbf{b}-A\\mathbf{x}^{(k)},\\quad k\\geq 0.\n$$\n"],"metadata":{}},{"cell_type":"markdown","source":["Alors, on peut réécrire [[eq2]](#eq2) sous la forme\n"],"metadata":{}},{"cell_type":"markdown","source":["\n$$\n\\label{eq:igra}\nP(\\mathbf{x}^{(k+1)}-\\mathbf{x}^{(k)})=\\mathbf{r}^{(k)},\\quad k\\geq 0.\n$$\n"],"metadata":{}},{"cell_type":"markdown","source":["Considérons la méthode modifiée, dite *la méthode du gradient* :\n"],"metadata":{}},{"cell_type":"markdown","source":["\n$$\n\\label{eq:rich}\nP(\\mathbf{x}^{(k+1)}-\\mathbf{x}^{(k)})=\\alpha_k\\mathbf{r}^{(k)},\\quad k\\geq 0,\n$$\n"],"metadata":{}},{"cell_type":"markdown","source":["où $\\alpha_k$ est un ***paramètre à choisir afin d&#8217;accélérer la convergence.***\n"],"metadata":{}},{"cell_type":"markdown","source":["Une formulation plus intèressante au niveau algorithmique est la suivante: on définit $\\mathbf{r}^0=\\mathbf{b}-A\\mathbf{x}^{(0)}$ de telle manière que chaque étape de la méthode de Richardson est donnée à partir des sous-itérations suivantes :\n"],"metadata":{}},{"cell_type":"markdown","source":["\n$$\n\\label{gradpre}\n\\begin{split}\nP\\mathbf{z}^{(k)}=\\mathbf{r}^{(k)}\\\\\n\\mathbf{x}^{(k+1)}=\\mathbf{x}^{(k)}+\\alpha_k\\mathbf{z}^{(k)}\\\\\n\\mathbf{r}^{(k+1)}=\\mathbf{r}^{(k)}-\\alpha_k A\\mathbf{z}^{(k)}\n\\end{split}\n$$\n\n","## Choix de $\\alpha_k$\n\n","On a les résultats suivants.\n","*Théorème: Cas stationnaire*\\\n","On suppose la matrice $P$ inversible et les valeurs propres de $P^{-1}A$ strictement positives et telles que latexmath:[\\lambda_{max}=\\lambda_1\\geq\\lambda_2\\geq \\ldots\\geq\n","\\lambda_n=\\lambda_{min}&gt;0]. Alors la méthode de Richardson stationnaire est convergente si et seulement si latexmath:[0&lt;\\alpha&lt;2/\n","\\lambda_1]. De plus, le rayon spectral de la matrice d&#8217;itération $R_\\alpha$ est minimal si $\\alpha=\\alpha_{opt}$\n\n","\n$$\n\\alpha_{opt}= \\frac{2}{\\lambda_{min}+\\lambda_{max}},\n$$\n\n","avec\n\n","\n$$\n\\rho_{opt}=\\frac{\\lambda_{max}-\\lambda_{min}}{\\lambda_{min}+\\lambda_{max}}\n$$\n","*Preuve*\\\n","La matrice d&#8217;itération de la méthode est donnée par $R_\\alpha=I-\\alpha P^{-1}A$, donc les valeurs propres de $R_\\alpha$ sont de la forme $1-\\alpha\\lambda_i$. La méthode sera convergente si et seulement si $|1-\\alpha\\lambda_i|<1$ pour $i=1,\\ldots,n$, soit donc si $-1<1-\\alpha\\lambda_i<1$ pour $i=1,\\ldots,n$. Comme $\\alpha>0$, ceci est équivalent à que $-1<1-\\alpha\\lambda_{max}$, d&#8217;où la condition necessaire et suffisante de convergence reste $\\alpha <2/  \\lambda_{max}$. Par conséquent, $\\rho(R_\\alpha)$ est minimal si $1-\\alpha\\lambda_n=\\alpha\\lambda_1-1$, c&#8217;est-à-dire, pour $\\alpha_{opt}=2/(\\lambda_{1}+\\lambda_{n})$. Par substitution, on obtient\n\n","\n$$\n\\rho_{opt}=\\rho(R_{opt})=1-\\alpha_{opt}\\lambda_{min}=\n1-\\frac{2\\lambda_{min}}{\\lambda_{min}+\\lambda_{max}}=\n\\frac{\\lambda_{max}-\\lambda_{min}}{\\lambda_{min}+\\lambda_{max}}\n$$\n\n","ce qui permet de completer la preuve. $\\Box$\n","*Théorème: Cas dynamique*\\\n","Si $A$ est symétrique définie positive, le choix optimal de $\\alpha_k$ est donné par\n\n","\n$$\n\\alpha_k=\\frac{(\\mathbf{r}^{(k)}, \\mathbf{z}^{(k)})}{(A\n\\mathbf{z}^{(k)}, \\mathbf{z}^{(k)})},\n\\quad\n\\quad k\\geq 0 \\quad,\n$$\n\n","où $\\mathbf{z}^{(k)}=P^{-1}\\mathbf{r}^{(k)}$.\n","*Preuve*\\\n","D&#8217;une part on a\n\n","\n$$\n\\mathbf{r}^{(k)}=\\mathbf{b}-A\\mathbf{x}^{(k)}=A(\\mathbf{x}-\\mathbf{x}^{(k)})=-A\\mathbf{e}^{(k)}\n$$\n\n","où $\\mathbf{e}^{(k)}$ represente l&#8217;erreur à l&#8217;étape $k$, et d&#8217;autre part\n\n","\n$$\n\\mathbf{e}^{(k+1)}=\\mathbf{e}^{(k+1)}(\\alpha)=\\underbrace{(I-\\alpha\nP^{-1}A)}_{\\displaystyle R_\\alpha}\\mathbf{e}^{(k)}.\n$$\n\n","En autre, grâce à la relation latexmath:[\\mathbf{x}^{(k+1)} =\n","\\mathbf{x}^{(k)} + \\alpha (P^{-1} \\mathbf{b} - P^{-1} A\n","\\mathbf{x}] on trouve:\n\n","\n$$\n\\begin{aligned}\n\\mathbf{r}^{(k+1)} &=& \\mathbf{b} - A \\mathbf{x}^{(k+1)} \\\\\n&=& \\mathbf{b} - A [ \\mathbf{x}^{(k)} + \\alpha (P^{-1} \\mathbf{b}\n- P^{-1} A \\mathbf{x}^{(k)} )]\\\\\n&=& \\mathbf{b} - A \\mathbf{x}^{(k)} - \\alpha A P^{-1} (\\mathbf{b}\n- A \\mathbf{x}^{(k)} )\\\\\n&=& \\mathbf{r}^{(k)} - \\alpha A P^{-1} \\mathbf{r}^{(k)}.\n\\end{aligned}\n$$\n\n","Donc, en notant avec $\\|\\cdot\\|_A$ la norme vectorielle issue du produit scalaire $(\\mathbf{x},\\mathbf{y})_A=(A\\mathbf{x},\\mathbf{y})$, c&#8217;est-à-dire, $\\|\\mathbf{x}\\|_A=(A\\mathbf{x},\\mathbf{x})^{1/2}$ on tire que\n\n","\n$$\n\\begin{aligned}\n\\|\\mathbf{e}^{(k+1)}\\|^2_A &=& (A\\mathbf{e}^{(k+1)},\\mathbf{e}^{(k+1)})=-(\\mathbf{r}^{(k+1)},\\mathbf{e}^{(k+1)})\\\\\n&=& - (\\mathbf{r}^{(k)} - \\alpha A P^{-1} \\mathbf{r}^{(k)},\n\\mathbf{e}^{(k)} - \\alpha P^{-1} A \\mathbf{e}^{(k)}) \\\\\n&=& -(\\mathbf{r}^{(k)},\\mathbf{e}^{(k)}) + \\alpha\n[(\\mathbf{r}^{(k)},P^{-1}A\\mathbf{e}^{(k)})+(A\\mathbf{z}^{(k)},\\mathbf{e}^{(k)})]\n- \\alpha^2(A\\mathbf{z}^{(k)},P^{-1}A\\mathbf{e}^{(k)})\n\\end{aligned}\n$$\n\n","Maintenant on choisi $\\alpha$ comme le $\\alpha_k$ qui minimise $\\|\\mathbf{e}^{(k+1)}(\\alpha)\\|_A$, c&#8217;est-à-dire,\n\n","\n$$\n\\left.\\frac{d}{d\\alpha}\\|\\mathbf{e}^{(k+1)}(\\alpha)\\|_A\\right|_{\\alpha=\\alpha_k}=0\n$$\n\n","On obtient donc\n\n","\n$$\n\\alpha_k=\\frac{1}{2}\n\\frac{(\\mathbf{r}^{(k)},P^{-1}A\\mathbf{e}^{(k)})+(A\\mathbf{z}^{(k)},\\mathbf{e}^{(k)})}{(A\\mathbf{z}^{(k)},P^{-1}A\\mathbf{e}^{(k)})}\n=\\frac{1}{2}\\frac{-(\\mathbf{r}^{(k)},\\mathbf{z}^{(k)})+\n(A\\mathbf{z}^{(k)},\\mathbf{e}^{(k)})}{-(A\\mathbf{z}^{(k)},\\mathbf{z}^{(k)})}\n$$\n\n","et grâce à l&#8217;égalité latexmath:[(A\\mathbf{z}^{(k)},\n","\\mathbf{e}^{(k)} ) = (\\mathbf{z}^{(k)}, A \\mathbf{e}^{(k)})] dûe au fait que $A$ est symétrique définie positive, et en remarquant que latexmath:[A\\mathbf{e}^{(k)} =\n","-\\mathbf{r}^{(k)}], on trouve\n\n","\n$$\n\\alpha_k=\\frac{(\\mathbf{r}^{(k)}, \\mathbf{z}^{(k)})}{(A\n   \\mathbf{z}^{(k)}, \\mathbf{z}^{(k)})} \\, ,\n$$\n\n","ce qui complète la démonstration. $\\Box$\n","En résumé, la méthode du gradient ($P=Id$) ou du gradient préconditionnée ($P \\neq Id$) peut donc s&#8217;écrire:\n","étant donné $\\mathbf{x}^{(0)} \\in \\mathbb{R}^n$, calculer latexmath:[\\mathbf{r}^{(0)} = \\mathbf{b} - A\n","\\mathbf{x}^{(0)}] et pour $k = 0, 1, \\ldots$ jusqu&#8217;à convergence\n","\n$$\n\\begin{array}{ll}\nP\\mathbf{z}^{(k)} = \\mathbf{r}^{(k)} & \\textrm{résidu\npréconditionné}\\\\\n\\displaystyle{\\alpha_k = \\frac{ (\\mathbf{r}^{(k)},\n\\mathbf{z}^{(k)})}{(A\n\\mathbf{z}^{(k)}, \\mathbf{z}^{(k)} )}} \\\\\n\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + \\alpha_k \\mathbf{z}^{(k)}\n& \\\\\n\\mathbf{r}^{(k+1)} = \\mathbf{r}^{(k)} - \\alpha_k A \\mathbf{z}^{(k)} & \\textrm{mise à jour du résidu}\n\\end{array}\n$$\n\n","## Convergence de la méthode du gradient\n\n","*Théorème: Convergence de la méthode du gradient préconditionnée*\\\n","Si $A$ est une matrice symétrique définie positive, alors la méthode de Richardson stationnaire préconditionnée est convergente et\n\n","\n$$\n\\| \\mathbf{e}^{(k+1)}\\|_A\n\\leq\\frac{K_2(P^{-1}A)-1}{K_2(P^{-1}A)+1}\\|\n\\mathbf{e}^{(k)}\\|_A, \\quad k\\geq 0,\n$$\n\n","si $P$, $A$ et $P^{-1}A$ sont symétriques définies positives. Le même résultat est valable pour la méthode du gradient préconditionnée._\n","*Preuve*\\\n","Voir Corollaire 4.1 p. 130, et Théorème 4.10 p. 139 du livre de Quarteroni et Sacco. $\\Box$\n","*Important:* ","En général, on choisit $P$ de façon à avoir\n","\n$$\nK_2(P^{-1}A) \\ll K_2(A).\n$$\n","En conséquence,\n","\n$$\n\\frac{K_2(P^{-1}A)-1}{K_2(P^{-1}A)+1} \\ll  \\frac{K_2(A)-1}{K_2(A)+1}.\n$$\n\n","## Pourquoi la \"méthode du gradient\"?\n\n","La fonctionnelle\n","\n$$\n\\Phi: \\mathbb{R}^n \\to \\mathbb{R}, \\quad\n  \\Phi(\\mathbf{x})=\\frac{1}{2}\\mathbf{x}^TA\\mathbf{x}-\\mathbf{x}^T\\mathbf{b}, \\quad\n  \\Phi(\\mathbf{x}) = \\frac{1}{2} \\sum_{i,j=1}^n x_i a_{ij}x_j - \\sum_{i=1}^n b_i x_i,\n$$\n","définit l&#8217;énergie du système $A\\mathbf{x}=\\mathbf{b}$. En effet, comme $A$ est symétrique\n","\n$$\n\\frac{\\partial \\Phi}{\\partial x_m} = \\frac{1}{2} \\left( \\sum_{j=1}^n a_{mj} x_j + \\sum_{i=1}^n x_i a_{im}\n  \\right) - b_m = \\frac{1}{2} \\left[ (A\\mathbf{x})_m + (A^T \\mathbf{x})_m \\right] - b_m = (A \\mathbf{x})_m -\n  b_m\n$$\n","\n$$\n\\Rightarrow \\quad \\nabla\\Phi(\\mathbf{x})=\\frac{1}{2}(A^T+A)\\mathbf{x}-\\mathbf{b}=A\\mathbf{x}-\\mathbf{b} \\, .\n$$\n","Par conséquent,\n","\n$$\n\\label{gradres}\n\\mathbf{r}(\\mathbf{x})=-\\nabla\\Phi(\\mathbf{x})\n$$\n","et si $\\nabla\\Phi(\\mathbf{x})=0$ alors $\\mathbf{x}$ est solution du système.\n","Inversement, si $\\mathbf{x}$ est solution du système alors $\\mathbf{x}$ minimise la fonctionnelle $\\Phi$, car comme $\\Phi$ est une fonctionnelle quadratique un développement de Taylor donne\n","\n$$\n\\begin{aligned}\n\\Phi(\\mathbf{y})=\\Phi(\\mathbf{x}+(\\mathbf{y}-\\mathbf{x}))= \\Phi ({\\bf x}) + \\frac{1}{2}\n({\\bf y} - {\\bf x})^T {\\rm A} ( {\\bf y} - {\\bf x} ),\n\\qquad \\forall {\\bf y} \\in \\mathbb{R}^n\n\\end{aligned}\n$$\n","et donc $\\Phi ( {\\bf y} ) > \\Phi ( {\\bf x} )$ pour ${\\bf y} \\neq {\\bf x}$.\n","Considèrons la méthode [[eq13]](#eq13) avec $P^{-1}$, à l&#8217;étape $k$, $\\mathbf{x}^{(k+1)}$ est déterminé par\n","\n$$\n\\mathbf{x}^{(k+1)}=\\mathbf{x}^{(k)} + \\alpha_k \\mathbf{d}^{(k)},\n$$\n","où $\\alpha_k$ est la valeur qui fixe la longueur du pas le long de $\\mathbf{d}^{(k)}$.\n","L&#8217;idée la plus naturelle est de prendre la direction de descente de pente maximale $\\nabla \\Phi(\\mathbf{x}^{(k)})$.\n","C&#8217;est la ***méthode du gradient*** ou ***méthode de plus profonde descente***.\n","D&#8217;après [l&#8217;équation ci-dessus](#eq16), $\\nabla  \\Phi ( \\mathbf{x}^{(k)} ) =  A\\mathbf{x}^{(k)}- \\mathbf{b} = -\\mathbf{r}^{(k)}$, la direction du gradient de $\\Phi$ coı̈ncide donc avec le résidu et peut être immédiatement calculée en utilisant la valeur $\\mathbf{x}^{(k)}$.\n","*Important:* Ceci montre que la méthode du gradient, comme celle de Richardson, revient à se déplacer à chaque étape $k$ le long de la direction $\\mathbf{d}^{(k)}=\\mathbf{r}^{(k)}$, avec un paramètre $\\alpha_k$ à déterminer.\n","## Implémentation de la méthode du gradient\n\n","Implémentons la méthode du gradient préconditionnée avec la documentation\n"],"metadata":{}},{"cell_type":"code","execution_count":0,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["Unresolved directive in 4-gradient.adoc - include::example$tan/syslin/gradient.py[]\n"]},{"cell_type":"markdown","source":["## Exemple\n\n","Appliquons la méthode du gradient préconditionnée à la résolution du système linéaire suivant:\n"],"metadata":{}},{"cell_type":"code","execution_count":0,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["import numpy as np\n","# pick a 4x4 matrix symétrique définie positive\n","A=np.array([[2.,-1.,0.,0.],[-1.,2.,-1.,0.],[0.,-1.,2.,-1.],[0.,0.,-1.,2.]])\n","print(f\"A = {A}\")\n","# on calcule les valeurs propres de A\n","print(f\"Valeurs propres de A: {np.linalg.eigvals(A)}\")\n","print(f\"Conditionnement de A: {np.linalg.cond(A)}\")\n"]},{"cell_type":"markdown","source":["D&#8217;après le théorème [[thm:3]](#thm:3), la méthode du gradient (ou de Richardson) préconditionnée converge.\n"],"metadata":{}},{"cell_type":"code","execution_count":0,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["from tan.syslin.gradient import gradient\n","[x_g,niter_g,res_g,inc_g]=gradient( A, np.array([1.,0.,1.,0]), P=np.diag(np.diag(A)) )\n","print(f\"Solution: x = {x_g}\")\n","print(f\"Nombre d'itérations: {niter_g}\")\n"]},{"cell_type":"markdown","source":["Nous pouvons tracer la convergence du résidu ainsi que celle de l&#8217;incrément avec `Plotly`\n"],"metadata":{}},{"cell_type":"code","execution_count":0,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["import plotly.graph_objects as go\n","fig = go.Figure()\n","fig.add_trace(go.Scatter(x=np.arange(0,niter_g+1), y=inc_g, mode='lines+markers', name='Incrément'))\n","fig.add_trace(go.Scatter(x=np.arange(0,niter_g+1), y=res_g, mode='lines+markers', name='Résidu'))\n","fig.update_layout(title='Convergence de la méthode du gradient préconditionnée', xaxis_title='Itération', yaxis_title='Incrément et Résidu', yaxis_type='log')\n","fig.show()\n"]}],"metadata":{"language_info":{"name":"python","version":"3.9.1"},"kernelspec":{"name":"python3","language":"python"}},"nbformat":4,"nbformat_minor":4}