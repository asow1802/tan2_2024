{"cells":[{"cell_type":"markdown","source":["# Série 5 : Méthodes itératives pour la résolution de systèmes linéaires\n\n","## Théorique\n\n","*Exercice 1*\\\n","On considère le système linéaire $A\\mathbf{x}=\\mathbf{b}$, où\n\n","\n$$\nA = \\begin{pmatrix}\n6 & -3 & 0  \\\\\n-3 & 6 & 4\\\\\n0  & 4 & 6\n\\end{pmatrix}, \\quad \\mathbf{b}=\\begin{pmatrix}1\\\\ 2\\\\ -3\\end{pmatrix}.\n$$\n\n","\n","1. Démontrer que, s&#8217;il existe une constante $0<C<1$ telle que, pour tout $k\\in \\mathbb{N}$,\n","\n$$\n\\|\\mathbf{x}^{(k+1)}-\\mathbf{x}\\|_{A} \\leq C\\| \\mathbf{x}^{(k)}-\\mathbf{x}\\|_{A} \\, ,\n$$\n","alors on a la majoration de l&#8217;erreur suivante (remarquez que l&#8217;estimation est indépendante de la solution $\\mathbf{x}$):\n","\n$$\n\\|\\mathbf{x}^{(k)}-\\mathbf{x}\\|_{A} \\leq \\frac{C^k}{1-C}\\|\n\\mathbf{x}^{(1)}-\\mathbf{x}^{(0)}\\|_{A}.\n$$\n","Sugg.: estimer $\\|\\mathbf{x}^{(0)}-\\mathbf{x}\\|_{A}$ par rapport à $\\|\\mathbf{x}^{(0)}-\\mathbf{x}^{(1)}\\|_{A}$.\n","\n$$\n\\|\\mathbf{x}^{(k+1)}-\\mathbf{x}\\|_{A} \\leq C\\| \\mathbf{x}^{(k)}-\\mathbf{x}\\|_{A}.\n$$\n","1. On considère la méthode de Richardson stationnaire préconditionné, avec matrice de préconditionnement $P=D$, $D$ étant la partie diagonale de $A$. La méthode est-elle convergente? Calculer le paramètre $\\alpha_{opt}$ optimal (qui caractérise la méthode du gradient stationnaire).\n","1. On considère maintenant la méthode du gradient préconditionné(Richardson dynamique), toujours avec préconditionneur $P=D$. La méthode est-elle convergente? Calculer le facteur $C$ de réduction de l&#8217;erreur, tel que\n","1. En choisissant comme vecteur initial $\\mathbf{x}^{(0)}=(0,0,0)^T$, estimer le nombre minimal d&#8217;itérations nécessaires pour avoir une erreur (en norme $\\|\\cdot \\|_{A}$) plus petite que $10^{-8}$.\n","\n","*Exercice 2*\\\n","On considère les systèmes linéaires latexmath:[A\n","\\mathbf{x}=\\mathbf{b}], où:\n\n","\n$$\nA = \\left(\n  \\begin{array}{ccccc}\n    2 & -1  &  & \\textrm{\\huge{0}} \\\\\n    -1 & 2 & \\ddots &   \\\\\n      & \\ddots &  \\ddots &  -1 \\\\\n    \\textrm{\\huge{0}} &  & -1 & 2\n  \\end{array}\n      \\right) \\in\\mathbb{R}^{10\\times 10} \\; ,\n  \\quad\n  \\mathbf{b} = A \\ \\left(\n  \\begin{array}{c}\n    1\\\\\n    1\\\\\n     \\vdots \\\\\n    1\n  \\end{array}\n             \\right)\\in\\mathbb{R}^{10}  .\n$$\n\n","\n","- Analyser le rayon spéctral de la matrice d&#8217;itération de la méthode SOR en fonction du paramètre de relaxation $\\omega=0,0.1,0.2,\\ldots,2$. Donner la valeur $\\omega_{opt}$ pour laquelle la méthode converge le plus vite dans cet exemple.\n","- En utilisant le fichier `sor`, comparer le nombre d&#8217;itérations effectuées par la méthode SOR en fonction de $\\omega$ pour achever une erreur en dessous d&#8217;une tolérance $\\verb|tol|=10^{-9}$ et $\\mathbf{x}_0 = (0, \\ldots, 0)^\\top$. Quel est le facteur qu&#8217;on peut gagner en nombre d&#8217;itérations si on utilise $\\omega=\\omega_{opt}$ calculé au point précédent par rapport à Gauss-Seidel ($\\omega=1$).\n","\n","*Exercice 3*\\\n","Considérons la méthode du gradient (dite aussi méthode de Richardson dynamique sans préconditionnement).\n\n","\n","1. Montrer que le paramètre d&#8217;accélération $\\alpha_k$ correspond à la solution du problème de minimisation suivant :\n","\n$$\n\\alpha_k=\\mbox{arg min}\\, \\Phi(\\mathbf{x}^{(k)}+\\alpha\\mathbf{r}^{(k)}),\n$$\n","où $\\Phi$ désigne l&#8217;énergie du système $A\\mathbf{x}=\\mathbf{b}$, définie par $\\Phi(\\mathbf{x})=\\tfrac12\\mathbf{x}^TA\\mathbf{x}-\\mathbf{x}^T\\mathbf{b}$. Dans ce cas on dit que $\\mathbf{x}^{(k)}$ est optimal par rapport à la direction $\\mathbf{r}^{(k)}$.\n","1. Montrer que $(\\mathbf{r}^{(k+1)},\\mathbf{r}^{(k)})=0$.\n","1. Interpréter géométriquement la méthode du gradient.\n","\n","*Exercice 4*\\\n","Il s&#8217;agit de résoudre le système linéaire $A\\mathbf{x}=\\mathbf{b}$, où $A$ et $\\mathbf{b}$ sont\n\n","\n$$\nA=\\begin{bmatrix}\n2 & -1\\\\\n-1 & 2\n\\end{bmatrix},\\quad\n\\mathbf{b}=\\begin{bmatrix}\n1\\\\\n1\n\\end{bmatrix}\n.\n$$\n\n","\n","- Vérifier que $\\operatorname{A}$ est symétrique définie positive.\n","- Effectuer deux itérations de la méthode du gradient conjugué en partant de $\\displaystyle\n\\mathbf{x}^{(0)}=\\begin{bmatrix}\n3/2\\\\\n2\n\\end{bmatrix}.$ Vérifier que $\\mathbf{x}^{(2)}=\\mathbf{x}$, où x est la solution du système linéaire $A\\mathbf{x}=\\mathbf{b}$.\n","- Soit $\\displaystyle \\Phi(\\mathbf{x})=\\frac{1}{2}(A\\mathbf{x},\\mathbf{x})-(\\mathbf{b},\\mathbf{x})$ la fonctionnelle d&#8217;énergie associée au système linéaire. Représenter graphiquement sur le plan $\\mathbb{R}^2$ les points $\\mathbf{x}^{(0)},\\mathbf{x}^{(1)}$ et $\\mathbf{x}^{(2)}$, ainsi que les lignes de niveau de $\\Phi$ passant par ces points.\n","\n","*Exercice 5*\\\n","Considérons la méthode du gradient conjugué sans préconditionnement.\n\n","\n","- Démontrer que le résidu $\\mathbf{r}^{(k+1)}$ est orthogonal aux diréctions $\\mathbf{p}^{(j)}$, $j=0,\\ldots,k$, c&#8217;est-à-dire\n","\n$$\n(\\mathbf{p}^{(j)},\\mathbf{r}^{(k+1)} )= 0 \\quad\\mbox{pour}\\quad j =0,\\ldots,k,\n$$\n","que les résidue sont tous orthogonales, c&#8217;est-à-dire\n","\n$$\n(\\mathbf{r}^{(j)},\\mathbf{r}^{(k+1)} )= 0 \\quad\\mbox{pour}\\quad j =0,\\ldots,k,\n$$\n","et que les directions successives sont $A$-orthogonales, c&#8217;est-à-dire\n","\n$$\n(\\mathbf{p}^{(j)},\\mathbf{p}^{(k+1)})_A =0 \\quad\\mbox{pour}\\quad j=0,\\ldots,k.\n$$\n","Pour faire cela procéder comme suit:\n","\n","- Démontrer d&#8217;abord l&#8217;hypothèse initiale\n","\n$$\n(\\mathbf{p}^{(0)},\\mathbf{r}^{(1)})=0, \\quad (\\mathbf{r}^{(0)},\\mathbf{r}^{(1)})=0 \\quad\\mbox{et} \\quad (\\mathbf{p}^{(0)},\\mathbf{p}^{(1)})_A=0.\n$$\n","\n$$\n\\begin{aligned}\n    &&(\\mathbf{p}^{(j)},\\mathbf{r}^{(k)})=0 \\quad\\mbox{pour}\\quad j=0,\\ldots,k-1,\\\\\n    &&(\\mathbf{r}^{(j)},\\mathbf{r}^{(k)})=0 \\quad\\mbox{pour}\\quad j=0,\\ldots,k-1,\\\\\n    &&(\\mathbf{p}^{(j)},\\mathbf{p}^{(k)})_A=0 \\quad\\mbox{pour}\\quad j=0,\\ldots,k-1,\n\\end{aligned}\n$$\n","et démontrer que\n","\n$$\n\\begin{aligned}\n    &&(\\mathbf{p}^{(j)},\\mathbf{r}^{(k+1)} )= 0 \\quad\\mbox{pour}\\quad j =0,\\ldots,k,\\\\\n    &&(\\mathbf{r}^{(j)},\\mathbf{r}^{(k+1)} )= 0 \\quad\\mbox{pour}\\quad j =0,\\ldots,k,\\\\\n    &&(\\mathbf{p}^{(j)},\\mathbf{p}^{(k+1)})_A=0 \\quad\\mbox{pour}\\quad j=0,\\ldots,k.\n\\end{aligned}\n$$\n","- Supposer l&#8217;hypothèse de récurrence suivante\n","- Conclure par récurrence.\n","\n","\n","## Python\n\n","*Exercice 6*\\\n","On veut résoudre le système linéaire\n\n","\n$$\nA \\mathbf{x} = \\mathbf{b}\n$$\n\n","où $A$ est la matrice de Hilbert de taille $5$ (commande `hilb(5)`), et $\\mathbf{b}$ est un vecteur tel que la solution du système soit $\\mathbf{x} = (1,1,1,1,1)^t$ (c&#8217;est-à-dire `ones(5,1)`). Vérifier s&#8217;il est possible d&#8217;appliquer la méthode du gradient et du gradient conjugué pour résoudre ce système.\n\n","\n","- Essayer de résoudre le système avec la méthode du gradient non préconditionné (Richardson dynamique). Utiliser  `tan.syslin.gradient`  avec le préconditionneur $I_5$ (la matrice identité de taille 5, `np.eye(5)`), une tolérance de $10^{-15}$, un vecteur initial $\\mathbf{x}^{(0)}=(0,0,0,0,0)^t$ et un nombre maximum d&#8217;itérations égal à $10^5$. Est-ce que la méthode converge?\n","- Utiliser maintenant le préconditionneur $P$ défini par la partie triangulaire inférieure de $A$ (commande `np.tril(A)`). Est-ce que la méthode du gradient préconditionné est convergente? Combien d&#8217;itérations sont nécessaires?\n","- Résoudre le même système avec la méthode du gradient conjugué ( `tan.syslin.pcg`) non-préconditionné. Combien d&#8217;itérations sont nécessaires? Comparez avec les points a) et b).\n","\n","*Exercice 7*\\\n","On considère les systèmes linéaires $A_\\epsilon\\mathbf{x}=\\mathbf{b}_\\epsilon$, où:\n\n","\n$$\nA_\\epsilon = \\left(\n  \\begin{array}{cccccc}\n    1 & \\epsilon & \\epsilon^2 &  & \\textrm{\\huge{0}} \\\\\n    \\epsilon & 1 & \\epsilon & \\ddots  &   \\\\\n    \\epsilon^2 & \\epsilon & \\ddots & \\ddots  & \\epsilon^2 \\\\\n     & \\ddots &  \\ddots &  1 &  \\epsilon \\\\\n    \\textrm{\\huge{0}} &  & \\epsilon^2 & \\epsilon & 1\n  \\end{array}\n      \\right) \\; ,\n  \\quad\n  \\mathbf{b}_\\epsilon = A_\\epsilon \\ \\left(\n  \\begin{array}{c}\n    1\\\\\n    1\\\\\n    1 \\\\\n    1 \\\\\n    \\vdots \\\\\n    1\n  \\end{array}\n             \\right), \\quad \\epsilon \\in [0,1] \\; .\n$$\n\n","Pour une taille `n` et une valeur `epsi` de $\\epsilon$ données, il est possible d&#8217;utiliser la commande\n"],"metadata":{}},{"cell_type":"code","execution_count":0,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["from tan.matrix import matrix\n","A, b = matrix(5, 0.1)\n","print(f\"A =\\n {A}\")\n","print(f\"b =\\n {b}\")\n"]},{"cell_type":"markdown","source":["pour construire la matrice $A_\\epsilon$ et le vecteur $\\mathbf{b}_\\epsilon$ du système. Dans la suite on considère `n=5`.\n","\n","- On considère la matrice `A =` $A_\\epsilon$ avec $\\epsilon=0.6$ et une taille relativement grande, notamment `n=1000`.\n","\n","- Quel est le rapport entre le nombre des éléments non-nuls de `A` et le nombre des éléments nuls (voir commande `nnz`)?\n","- Le format de stockage `sparse` (fr. creux) permet de mémoriser seulement les éléments non-nuls d&#8217;une matrice. Serait-il utile de mémoriser `A` avec un tel format? Vérifier votre réponse en comparant l&#8217;occupation mémoire (utiliser `whos` sous ipython après avoir défini la matrice aux formats creux et dense) de `A` et de sa copie de type `sparse`, `As = csr_matrix(A)`.\n"],"metadata":{}},{"cell_type":"code","execution_count":0,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["import numpy as np\n","from scipy.sparse import csr_matrix\n","\n","# Créer une matrice dense avec de nombreux zéros\n","dense_matrix = np.array([\n","    [1, 0, 0, 0, 2],\n","    [0, 0, 3, 0, 0],\n","    [4, 0, 0, 0, 5],\n","    [0, 6, 0, 0, 0],\n","    [7, 0, 0, 8, 9]\n","])\n","\n","# Convertir la matrice dense en une matrice Compressed Sparse Row (CSR)\n","sparse_matrix = csr_matrix(dense_matrix)\n","\n","print(sparse_matrix)\n"]},{"cell_type":"markdown","source":["vous pouvez donc utiliser la commande `csr_matrix` pour convertir une matrice dense en une matrice creuse et inversement la commande `todense()` pour convertir une matrice creuse en une matrice dense.\n"],"metadata":{}},{"cell_type":"code","execution_count":0,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["# Convertir la matrice éparse en une matrice dense\n","dense_matrix = sparse_matrix.todense()\n","print(dense_matrix)\n"]},{"cell_type":"markdown","source":["- Calculer le rapport entre les temps de calcul de la solution $\\mathbf{x}$ avec les deux formats de stockage, lorsque on utilise la méthode de Richardson stationnaire (commande `richardson`), avec le paramètre $\\alpha=0.3$, une tolérance de $\\verb|Tol| = 10^{-9}$, $\\mathbf{x_0}=(0,\\ldots,0)^\\top$ et sans préconditionnement (c&#8217;est-à-dire $P=I_n$). Chaque temps peut se calculer avec une ligne du type\n"],"metadata":{"node_name":"ulist"}},{"cell_type":"code","execution_count":0,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["from pytictoc import tic, toc # <1>\n","t = TicToc() # <2>\n","t.tic() # <3>\n","x=richardson(...)\n","t.toc() # <4>\n"]},{"cell_type":"markdown","source":["\n1. il faut importer les commandes `+tic+` et `+toc+` du module `+pytictoc+` (commande `+pip install pytictoc+`).","\n2. on crée un objet `+t+` de type `+TicToc+`.","\n3. on démarre le chronomètre.","\n4. on arrête le chronomètre et on affiche le temps écoulé.","","où `tic` et `toc` sont les commandes chronomètre de Python. N&#8217;oubliez pas de également mettre la matrice de préconditionnement $P$ sous forme `sparse`.\n","\n"],"metadata":{"node_name":"colist"}},{"cell_type":"markdown","source":["- On considère finalement la matrice `A =` $A_\\epsilon$ avec $\\epsilon=0.5$ et `n=10`. En utilisant la commande `eig` calculer les valeurs propres de $A$. Puis tracer le nombre d&#8217;itérations de la méthode de Richardson stationnaire en fonction de $\\alpha\\in (0,2/{\\lambda_{max}})$.\n","\n\n","*Note:* ipython est un shell interactif de Python. Pour lancer ipython il suffit de taper `ipython` dans un terminal. Pour lancer ipython avec un notebook il suffit de taper `ipython notebook` dans un terminal. Pour lancer ipython avec un notebook et un support pour le calcul scientifique il suffit de taper `ipython notebook --pylab=inline` dans un terminal. Pour plus d&#8217;informations sur ipython voir `http://ipython.org/`.\n"],"metadata":{"node_name":"ulist"}}],"metadata":{"language_info":{"name":"python","version":"3.9.1"},"kernelspec":{"name":"python3","language":"python"}},"nbformat":4,"nbformat_minor":4}